{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68b16bb",
   "metadata": {},
   "source": [
    "# üéØ ƒê√ÅNH GI√Å H·ªÜ TH·ªêNG G·ª¢I √ù S·∫¢N PH·∫®M\n",
    "# Recommendation Engine Evaluation\n",
    "\n",
    "## T·ªïng Quan / Overview\n",
    "Notebook n√†y ƒë√°nh gi√° to√†n di·ªán **H·ªá th·ªëng G·ª£i √Ω S·∫£n ph·∫©m** cho JanSport E-commerce Store:\n",
    "This notebook provides comprehensive evaluation of the **Recommendation Engine** for JanSport E-commerce:\n",
    "\n",
    "### üìã N·ªôi dung ƒë√°nh gi√° / Evaluation Coverage:\n",
    "- **‚ö° Hi·ªáu nƒÉng API / API Performance**: Response time, throughput, cache hit rate c·ªßa 5 chi·∫øn l∆∞·ª£c\n",
    "- **üéØ ƒê·ªô ch√≠nh x√°c / Accuracy**: Precision, Recall, F1-score cho hybrid algorithm\n",
    "- **üìä Ch·∫•t l∆∞·ª£ng recommendation**: Click-through rate, Conversion rate tr√™n s·∫£n ph·∫©m JanSport\n",
    "- **‚ùÑÔ∏è X·ª≠ l√Ω Cold Start**: New user/product scenarios v·ªõi 100+ s·∫£n ph·∫©m JanSport\n",
    "- **üîÑ Stability & Load**: Load testing, error handling v·ªõi PostgreSQL + Redis\n",
    "\n",
    "### üéØ M·ª•c ti√™u hi·ªáu nƒÉng / Performance Targets:\n",
    "- **Response time**: < 500ms (P95) cho cached, < 2s cho computed\n",
    "- **Cache hit rate**: > 80% v·ªõi Redis TTL 1 gi·ªù  \n",
    "- **Click-through rate**: > 2% (business metric)\n",
    "- **Cold start coverage**: > 90% cho user m·ªõi\n",
    "\n",
    "### üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng / System Architecture:\n",
    "- **FastAPI Service** (Port 8001): Python 3.11, async endpoints\n",
    "- **PostgreSQL Database**: 7 b·∫£ng recommendation (interactions, preferences, similarities, etc.)\n",
    "- **Redis Cache**: High-speed caching v·ªõi TTL optimization\n",
    "- **ML Hybrid Engine**: 40% Content-based + 60% Collaborative Filtering\n",
    "- **5 Recommendation Strategies**: Hybrid, Content, Collaborative, Trending, Frequently Bought Together\n",
    "\n",
    "---\n",
    "**üìÖ Ng√†y / Date:** December 18, 2025  \n",
    "**üè∑Ô∏è Phi√™n b·∫£n / Version:** 1.2  \n",
    "**üéØ D·ªãch v·ª• / Service:** Recommendation Engine (Port 8001)  \n",
    "**üõçÔ∏è C·ª≠a h√†ng / Store:** JanSport Backpacks & Accessories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20124c87",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b03053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for JanSport Recommendation Engine evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For database connection (PostgreSQL shared with Medusa)\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import redis\n",
    "\n",
    "# For metrics calculation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# JanSport E-commerce Recommendation Service Configuration\n",
    "RECOMMENDATION_SERVICE_URL = \"http://localhost:8001\"  # FastAPI Recommendation Service\n",
    "MEDUSA_SERVICE_URL = \"http://localhost:9000\"          # Medusa v2 Backend for product data\n",
    "\n",
    "# Shared PostgreSQL Database Configuration (same as Medusa)\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'medusa-store',  # Shared database v·ªõi Medusa backend\n",
    "    'user': 'postgres',\n",
    "    'password': 'supersecretpassword'\n",
    "}\n",
    "\n",
    "# Redis Configuration (for caching)\n",
    "REDIS_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 6379,\n",
    "    'db': 0,  # Database 0 for recommendations\n",
    "    'decode_responses': True\n",
    "}\n",
    "\n",
    "print(\"üéØ ƒê√ÅNH GI√Å H·ªÜ TH·ªêNG G·ª¢I √ù S·∫¢N PH·∫®M - JANSPORT E-COMMERCE\")\n",
    "print(\"üéØ Recommendation Engine Evaluation - JanSport E-commerce\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÖ Ng√†y ƒë√°nh gi√° / Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ D·ªãch v·ª• ƒë√≠ch / Target Service: {RECOMMENDATION_SERVICE_URL} (FastAPI)\")\n",
    "print(f\"üõçÔ∏è Backend s·∫£n ph·∫©m / Product Backend: {MEDUSA_SERVICE_URL} (Medusa v2)\")\n",
    "print(f\"üóÑÔ∏è C∆° s·ªü d·ªØ li·ªáu / Database: {DB_CONFIG['host']}:{DB_CONFIG['port']} ({DB_CONFIG['database']})\")\n",
    "print(f\"üîÑ Cache Redis: {REDIS_CONFIG['host']}:{REDIS_CONFIG['port']} (DB {REDIS_CONFIG['db']})\")\n",
    "print(f\"üíº C·ª≠a h√†ng / Store: JanSport Backpacks & Accessories (100+ products)\")\n",
    "print(f\"üéØ ML Algorithm: Hybrid (40% Content + 60% Collaborative Filtering)\")\n",
    "print(f\"üìä Strategies: 5 recommendation strategies (Hybrid, Content, Collaborative, Trending, Together)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84a36e",
   "metadata": {},
   "source": [
    "## 2. Database Connection and Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and validate schema\n",
    "try:\n",
    "    # PostgreSQL connection\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # Redis connection\n",
    "    redis_client = redis.from_url(REDIS_URL)\n",
    "    redis_client.ping()\n",
    "    \n",
    "    print(\"‚úÖ Database connections established successfully!\")\n",
    "    \n",
    "    # Check recommendation tables\n",
    "    rec_tables_query = \"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public' \n",
    "    AND table_name LIKE 'rec_%'\n",
    "    ORDER BY table_name;\n",
    "    \"\"\"\n",
    "    \n",
    "    rec_tables = pd.read_sql(rec_tables_query, conn)\n",
    "    print(f\"\\nüìä Found {len(rec_tables)} recommendation tables:\")\n",
    "    for table in rec_tables['table_name']:\n",
    "        print(f\"  - {table}\")\n",
    "    \n",
    "    # Validate table structures\n",
    "    for table in rec_tables['table_name']:\n",
    "        count_query = f\"SELECT COUNT(*) as count FROM {table};\"\n",
    "        count_result = pd.read_sql(count_query, conn)\n",
    "        print(f\"  üìà {table}: {count_result['count'].iloc[0]} records\")\n",
    "    \n",
    "    # Check service health\n",
    "    health_response = requests.get(f\"{RECOMMENDATION_SERVICE_URL}/health\", timeout=5)\n",
    "    if health_response.status_code == 200:\n",
    "        health_data = health_response.json()\n",
    "        print(f\"\\n‚úÖ Recommendation service is healthy!\")\n",
    "        print(f\"üìä Service status: {health_data}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Service health check failed: {health_response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db65ea",
   "metadata": {},
   "source": [
    "## 3. Load Test Data for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba19ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data and generate test scenarios for JanSport products\n",
    "print(\"üìä Loading existing recommendation data for JanSport E-commerce...\")\n",
    "\n",
    "try:\n",
    "    # Load user interactions\n",
    "    interactions_query = \"\"\"\n",
    "    SELECT \n",
    "        user_id, \n",
    "        session_id, \n",
    "        product_id,\n",
    "        product_handle,\n",
    "        interaction_type,\n",
    "        weight,\n",
    "        created_at,\n",
    "        metadata\n",
    "    FROM rec_user_interactions\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 1000;\n",
    "    \"\"\"\n",
    "\n",
    "    interactions_df = pd.read_sql(interactions_query, conn)\n",
    "    print(f\"‚úÖ Loaded {len(interactions_df)} user interactions\")\n",
    "    \n",
    "    if len(interactions_df) > 0:\n",
    "        print(f\"üìà Interaction types: {interactions_df['interaction_type'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Load user preferences  \n",
    "    preferences_query = \"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        category,\n",
    "        score,\n",
    "        interaction_count,\n",
    "        last_updated\n",
    "    FROM rec_user_preferences\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 100;\n",
    "    \"\"\"\n",
    "\n",
    "    preferences_df = pd.read_sql(preferences_query, conn)\n",
    "    print(f\"‚úÖ Loaded {len(preferences_df)} user preferences\")\n",
    "    \n",
    "    if len(preferences_df) > 0:\n",
    "        print(f\"üéØ Top categories: {preferences_df['category'].value_counts().head().to_dict()}\")\n",
    "\n",
    "    # Load JanSport products for testing\n",
    "    products_query = \"\"\"\n",
    "    SELECT \n",
    "        p.id,\n",
    "        p.handle,\n",
    "        p.title,\n",
    "        p.collection_id,\n",
    "        p.thumbnail,\n",
    "        p.created_at,\n",
    "        pv.title as variant_title,\n",
    "        pv.id as variant_id,\n",
    "        ma.amount as price\n",
    "    FROM product p\n",
    "    LEFT JOIN product_variant pv ON p.id = pv.product_id\n",
    "    LEFT JOIN money_amount ma ON pv.id = ma.variant_id\n",
    "    WHERE p.title ILIKE '%JanSport%' OR p.handle ILIKE '%jansport%'\n",
    "    ORDER BY p.created_at DESC\n",
    "    LIMIT 50;\n",
    "    \"\"\"\n",
    "\n",
    "    products_df = pd.read_sql(products_query, conn)\n",
    "    print(f\"‚úÖ Loaded {len(products_df)} JanSport products for testing\")\n",
    "    \n",
    "    if len(products_df) > 0:\n",
    "        print(f\"üéí Sample products:\")\n",
    "        for _, product in products_df.head(3).iterrows():\n",
    "            price = f\"${product['price']/100:.2f}\" if product['price'] else \"N/A\"\n",
    "            print(f\"   ‚Ä¢ {product['title']} ({product['handle']}) - {price}\")\n",
    "\n",
    "    # Generate test user profiles for different scenarios\n",
    "    test_users = [\n",
    "        {\n",
    "            'user_id': 'test_user_new',\n",
    "            'profile': 'new_user',\n",
    "            'description': 'Ng∆∞·ªùi d√πng m·ªõi, ch∆∞a c√≥ l·ªãch s·ª≠ t∆∞∆°ng t√°c',\n",
    "            'expected_strategy': 'trending'\n",
    "        },\n",
    "        {\n",
    "            'user_id': 'test_user_student', \n",
    "            'profile': 'student',\n",
    "            'description': 'H·ªçc sinh th√≠ch balo school backpack',\n",
    "            'expected_strategy': 'content_based',\n",
    "            'preferences': ['backpack', 'school']\n",
    "        },\n",
    "        {\n",
    "            'user_id': 'test_user_traveler',\n",
    "            'profile': 'traveler', \n",
    "            'description': 'Du kh√°ch th√≠ch balo l·ªõn, laptop bag',\n",
    "            'expected_strategy': 'hybrid',\n",
    "            'preferences': ['travel', 'laptop', 'large']\n",
    "        },\n",
    "        {\n",
    "            'user_id': 'test_user_frequent',\n",
    "            'profile': 'frequent_buyer',\n",
    "            'description': 'Kh√°ch h√†ng th∆∞·ªùng xuy√™n, ƒë√£ mua nhi·ªÅu l·∫ßn',\n",
    "            'expected_strategy': 'collaborative',\n",
    "            'interaction_count': 50\n",
    "        },\n",
    "        {\n",
    "            'user_id': 'test_user_premium',\n",
    "            'profile': 'premium_customer',\n",
    "            'description': 'Kh√°ch h√†ng VIP, th√≠ch s·∫£n ph·∫©m cao c·∫•p',\n",
    "            'expected_strategy': 'hybrid',\n",
    "            'price_range': 'high'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    test_users_df = pd.DataFrame(test_users)\n",
    "    \n",
    "    # Generate test product scenarios\n",
    "    if len(products_df) > 0:\n",
    "        test_products = products_df.head(10).copy()\n",
    "        test_products['test_scenario'] = [\n",
    "            'popular_item', 'new_item', 'sale_item', 'trending_item', 'seasonal_item',\n",
    "            'premium_item', 'basic_item', 'limited_item', 'gift_item', 'bestseller_item'\n",
    "        ][:len(test_products)]\n",
    "    else:\n",
    "        # Fallback test products if no real products found\n",
    "        test_products = pd.DataFrame([\n",
    "            {'id': 'prod_test_1', 'handle': 'jansport-superbreak', 'title': 'JanSport Superbreak', 'test_scenario': 'popular_item'},\n",
    "            {'id': 'prod_test_2', 'handle': 'jansport-right-pack', 'title': 'JanSport Right Pack', 'test_scenario': 'new_item'},\n",
    "            {'id': 'prod_test_3', 'handle': 'jansport-big-student', 'title': 'JanSport Big Student', 'test_scenario': 'trending_item'},\n",
    "        ])\n",
    "\n",
    "    print(f\"\\nüß™ Test Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Test users: {len(test_users_df)} profiles\")\n",
    "    print(f\"   ‚Ä¢ Test products: {len(test_products)} products\")\n",
    "    print(f\"   ‚Ä¢ User profiles: {', '.join(test_users_df['profile'].tolist())}\")\n",
    "    \n",
    "    # Display test summary\n",
    "    print(f\"\\nüìã Test Data Summary:\")\n",
    "    print(f\"   ‚Ä¢ Historical interactions: {len(interactions_df)}\")\n",
    "    print(f\"   ‚Ä¢ User preferences: {len(preferences_df)}\")\n",
    "    print(f\"   ‚Ä¢ Available products: {len(products_df)}\")\n",
    "    print(f\"   ‚Ä¢ Test scenarios ready: ‚úÖ\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading test data: {str(e)}\")\n",
    "    # Create minimal test data for demo\n",
    "    test_users_df = pd.DataFrame([\n",
    "        {'user_id': 'demo_user', 'profile': 'demo', 'description': 'Demo user for testing'}\n",
    "    ])\n",
    "    test_products = pd.DataFrame([\n",
    "        {'id': 'demo_prod', 'handle': 'demo-product', 'title': 'Demo Product', 'test_scenario': 'demo_item'}\n",
    "    ])\n",
    "    interactions_df = pd.DataFrame()\n",
    "    preferences_df = pd.DataFrame()\n",
    "    products_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b605b",
   "metadata": {},
   "source": [
    "## 4. Recommendation Engine Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff56c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different recommendation strategies\n",
    "strategies = ['hybrid', 'content', 'collaborative', 'trending', 'frequently_bought_together']\n",
    "performance_results = []\n",
    "\n",
    "print(\"üß™ Testing recommendation strategies...\")\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nüìä Testing strategy: {strategy}\")\n",
    "    strategy_results = []\n",
    "    \n",
    "    # Test with different user types\n",
    "    for user_type, users in test_scenarios.items():\n",
    "        if user_type == 'session_ids':\n",
    "            continue\n",
    "            \n",
    "        for user_id in users[:5]:  # Test 5 users per type\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Make recommendation request\n",
    "                response = requests.get(\n",
    "                    f\"{RECOMMENDATION_SERVICE_URL}/recommendations\",\n",
    "                    params={\n",
    "                        'userId': user_id,\n",
    "                        'limit': 10,\n",
    "                        'algorithm': strategy\n",
    "                    },\n",
    "                    timeout=10\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                response_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    recommendations = data.get('recommendations', [])\n",
    "                    \n",
    "                    strategy_results.append({\n",
    "                        'strategy': strategy,\n",
    "                        'user_type': user_type,\n",
    "                        'user_id': user_id,\n",
    "                        'response_time_ms': response_time,\n",
    "                        'recommendation_count': len(recommendations),\n",
    "                        'status': 'success',\n",
    "                        'personalized': data.get('personalized', False),\n",
    "                        'cached': data.get('cached', False)\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    strategy_results.append({\n",
    "                        'strategy': strategy,\n",
    "                        'user_type': user_type,\n",
    "                        'user_id': user_id,\n",
    "                        'response_time_ms': response_time,\n",
    "                        'recommendation_count': 0,\n",
    "                        'status': 'error',\n",
    "                        'personalized': False,\n",
    "                        'cached': False\n",
    "                    })\n",
    "                    \n",
    "                # Small delay between requests\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error testing {strategy} for {user_id}: {str(e)}\")\n",
    "                strategy_results.append({\n",
    "                    'strategy': strategy,\n",
    "                    'user_type': user_type,\n",
    "                    'user_id': user_id,\n",
    "                    'response_time_ms': 0,\n",
    "                    'recommendation_count': 0,\n",
    "                    'status': 'timeout',\n",
    "                    'personalized': False,\n",
    "                    'cached': False\n",
    "                })\n",
    "    \n",
    "    performance_results.extend(strategy_results)\n",
    "    print(f\"‚úÖ Completed {len(strategy_results)} tests for {strategy}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "performance_df = pd.DataFrame(performance_results)\n",
    "print(f\"\\nüìä Performance testing completed!\")\n",
    "print(f\"   Total tests: {len(performance_df)}\")\n",
    "print(f\"   Success rate: {(performance_df['status'] == 'success').sum() / len(performance_df) * 100:.2f}%\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nüìà Response Time Statistics (ms):\")\n",
    "success_df = performance_df[performance_df['status'] == 'success']\n",
    "if len(success_df) > 0:\n",
    "    print(success_df.groupby('strategy')['response_time_ms'].agg(['mean', 'median', 'std', 'min', 'max']).round(2))\n",
    "\n",
    "display(performance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8a4e3",
   "metadata": {},
   "source": [
    "## 5. Interaction Tracking Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interaction tracking functionality\n",
    "print(\"üîç Testing interaction tracking...\")\n",
    "\n",
    "interaction_types = ['view', 'add_to_cart', 'purchase', 'wishlist_add', 'wishlist_remove']\n",
    "tracking_results = []\n",
    "\n",
    "# Record initial interaction count\n",
    "initial_count_query = \"SELECT COUNT(*) as count FROM rec_user_interactions;\"\n",
    "initial_count = pd.read_sql(initial_count_query, conn)['count'].iloc[0]\n",
    "print(f\"üìä Initial interactions in database: {initial_count}\")\n",
    "\n",
    "# Test interaction tracking\n",
    "for i, user_id in enumerate(test_scenarios['random_users'][:10]):\n",
    "    for j, interaction_type in enumerate(interaction_types):\n",
    "        try:\n",
    "            # Select a random product\n",
    "            product_id = products_df.iloc[np.random.randint(0, len(products_df))]['id']\n",
    "            product_handle = products_df[products_df['id'] == product_id]['handle'].iloc[0]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Track interaction\n",
    "            response = requests.post(\n",
    "                f\"{RECOMMENDATION_SERVICE_URL}/track\",\n",
    "                json={\n",
    "                    'user_id': user_id,\n",
    "                    'session_id': f'test_session_{i}',\n",
    "                    'product_id': product_id,\n",
    "                    'product_handle': product_handle,\n",
    "                    'interaction_type': interaction_type,\n",
    "                    'metadata': {\n",
    "                        'category': 'backpack',\n",
    "                        'price': np.random.randint(500000, 2000000),\n",
    "                        'test_run': True\n",
    "                    }\n",
    "                },\n",
    "                timeout=5\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = (end_time - start_time) * 1000\n",
    "            \n",
    "            tracking_results.append({\n",
    "                'user_id': user_id,\n",
    "                'product_id': product_id,\n",
    "                'interaction_type': interaction_type,\n",
    "                'response_time_ms': response_time,\n",
    "                'status_code': response.status_code,\n",
    "                'success': response.status_code == 200\n",
    "            })\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                logger.info(f\"‚úÖ Tracked {interaction_type} for {user_id}: {result}\")\n",
    "            else:\n",
    "                logger.error(f\"‚ùå Failed to track {interaction_type} for {user_id}: {response.status_code}\")\n",
    "                \n",
    "            time.sleep(0.05)  # Small delay\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error tracking interaction: {str(e)}\")\n",
    "            tracking_results.append({\n",
    "                'user_id': user_id,\n",
    "                'product_id': product_id,\n",
    "                'interaction_type': interaction_type,\n",
    "                'response_time_ms': 0,\n",
    "                'status_code': 0,\n",
    "                'success': False\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "tracking_df = pd.DataFrame(tracking_results)\n",
    "\n",
    "# Verify data was stored\n",
    "final_count_query = \"SELECT COUNT(*) as count FROM rec_user_interactions;\"\n",
    "final_count = pd.read_sql(final_count_query, conn)['count'].iloc[0]\n",
    "new_interactions = final_count - initial_count\n",
    "\n",
    "print(f\"\\nüìä Tracking Results:\")\n",
    "print(f\"   Total tracking attempts: {len(tracking_df)}\")\n",
    "print(f\"   Successful tracks: {tracking_df['success'].sum()}\")\n",
    "print(f\"   Success rate: {tracking_df['success'].mean() * 100:.2f}%\")\n",
    "print(f\"   New interactions in DB: {new_interactions}\")\n",
    "print(f\"   Avg response time: {tracking_df[tracking_df['success']]['response_time_ms'].mean():.2f} ms\")\n",
    "\n",
    "# Display tracking statistics by interaction type\n",
    "print(f\"\\nüìà Tracking by interaction type:\")\n",
    "tracking_stats = tracking_df.groupby('interaction_type').agg({\n",
    "    'success': ['count', 'sum', 'mean'],\n",
    "    'response_time_ms': 'mean'\n",
    "}).round(2)\n",
    "print(tracking_stats)\n",
    "\n",
    "display(tracking_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b86b6b",
   "metadata": {},
   "source": [
    "## 6. Response Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed response time analysis and visualization\n",
    "print(\"‚è±Ô∏è Performing detailed response time analysis...\")\n",
    "\n",
    "# Filter successful requests for analysis\n",
    "success_performance = performance_df[performance_df['status'] == 'success'].copy()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Response time by strategy\n",
    "sns.boxplot(data=success_performance, x='strategy', y='response_time_ms', ax=axes[0,0])\n",
    "axes[0,0].set_title('Response Time Distribution by Strategy')\n",
    "axes[0,0].set_xlabel('Strategy')\n",
    "axes[0,0].set_ylabel('Response Time (ms)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Response time by user type\n",
    "sns.boxplot(data=success_performance, x='user_type', y='response_time_ms', ax=axes[0,1])\n",
    "axes[0,1].set_title('Response Time Distribution by User Type')\n",
    "axes[0,1].set_xlabel('User Type')\n",
    "axes[0,1].set_ylabel('Response Time (ms)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Cache hit analysis\n",
    "cache_analysis = success_performance.groupby(['strategy', 'cached']).size().unstack(fill_value=0)\n",
    "cache_analysis.plot(kind='bar', ax=axes[1,0], stacked=True)\n",
    "axes[1,0].set_title('Cache Hit/Miss by Strategy')\n",
    "axes[1,0].set_xlabel('Strategy')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].legend(['Not Cached', 'Cached'])\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Personalization rate\n",
    "personalization = success_performance.groupby('strategy')['personalized'].mean()\n",
    "personalization.plot(kind='bar', ax=axes[1,1], color='lightcoral')\n",
    "axes[1,1].set_title('Personalization Rate by Strategy')\n",
    "axes[1,1].set_xlabel('Strategy')\n",
    "axes[1,1].set_ylabel('Personalization Rate')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate key performance metrics\n",
    "print(f\"\\nüìä Key Performance Metrics:\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\nüéØ Overall Performance:\")\n",
    "overall_stats = success_performance['response_time_ms'].describe()\n",
    "print(f\"   Mean response time: {overall_stats['mean']:.2f} ms\")\n",
    "print(f\"   Median response time: {overall_stats['50%']:.2f} ms\")\n",
    "print(f\"   95th percentile: {success_performance['response_time_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"   99th percentile: {success_performance['response_time_ms'].quantile(0.99):.2f} ms\")\n",
    "\n",
    "# Cache performance\n",
    "cache_hit_rate = success_performance['cached'].mean()\n",
    "print(f\"\\nüíæ Cache Performance:\")\n",
    "print(f\"   Cache hit rate: {cache_hit_rate:.2%}\")\n",
    "print(f\"   Avg cached response time: {success_performance[success_performance['cached']]['response_time_ms'].mean():.2f} ms\")\n",
    "print(f\"   Avg non-cached response time: {success_performance[~success_performance['cached']]['response_time_ms'].mean():.2f} ms\")\n",
    "\n",
    "# Strategy comparison\n",
    "print(f\"\\nüìà Strategy Performance Comparison:\")\n",
    "strategy_stats = success_performance.groupby('strategy').agg({\n",
    "    'response_time_ms': ['mean', 'median'],\n",
    "    'cached': 'mean',\n",
    "    'personalized': 'mean',\n",
    "    'recommendation_count': 'mean'\n",
    "}).round(2)\n",
    "strategy_stats.columns = ['Avg_Time_ms', 'Median_Time_ms', 'Cache_Hit_Rate', 'Personalization_Rate', 'Avg_Recommendations']\n",
    "print(strategy_stats)\n",
    "\n",
    "# Performance goals assessment\n",
    "print(f\"\\nüéØ Performance Goals Assessment:\")\n",
    "target_p95 = 500  # ms\n",
    "actual_p95 = success_performance['response_time_ms'].quantile(0.95)\n",
    "target_cache_hit = 0.80\n",
    "actual_cache_hit = cache_hit_rate\n",
    "\n",
    "print(f\"   P95 Response Time: {actual_p95:.2f}ms (Target: <{target_p95}ms) {'‚úÖ' if actual_p95 < target_p95 else '‚ùå'}\")\n",
    "print(f\"   Cache Hit Rate: {actual_cache_hit:.2%} (Target: >{target_cache_hit:.0%}) {'‚úÖ' if actual_cache_hit > target_cache_hit else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf923fd2",
   "metadata": {},
   "source": [
    "## 7. Accuracy Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90539c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recommendation accuracy metrics\n",
    "print(\"üéØ Calculating recommendation accuracy metrics...\")\n",
    "\n",
    "# Simulate ground truth data based on user interactions\n",
    "# In a real scenario, this would be based on actual click/purchase data\n",
    "def generate_ground_truth(user_interactions, top_n=10):\n",
    "    \"\"\"Generate ground truth recommendations based on user behavior\"\"\"\n",
    "    user_product_matrix = {}\n",
    "    \n",
    "    for _, interaction in user_interactions.iterrows():\n",
    "        user_id = interaction['user_id']\n",
    "        product_id = interaction['product_id']\n",
    "        weight = interaction['weight']\n",
    "        \n",
    "        if user_id not in user_product_matrix:\n",
    "            user_product_matrix[user_id] = {}\n",
    "        \n",
    "        if product_id not in user_product_matrix[user_id]:\n",
    "            user_product_matrix[user_id][product_id] = 0\n",
    "        \n",
    "        user_product_matrix[user_id][product_id] += weight\n",
    "    \n",
    "    # Generate ground truth: top products for each user\n",
    "    ground_truth = {}\n",
    "    for user_id, products in user_product_matrix.items():\n",
    "        sorted_products = sorted(products.items(), key=lambda x: x[1], reverse=True)\n",
    "        ground_truth[user_id] = [prod_id for prod_id, _ in sorted_products[:top_n]]\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "# Generate ground truth from existing interactions\n",
    "ground_truth = generate_ground_truth(interactions_df)\n",
    "print(f\"üìä Generated ground truth for {len(ground_truth)} users\")\n",
    "\n",
    "# Test recommendation accuracy for users with existing interactions\n",
    "accuracy_results = []\n",
    "test_users = list(ground_truth.keys())[:20]  # Test with 20 users\n",
    "\n",
    "for user_id in test_users:\n",
    "    try:\n",
    "        # Get recommendations for this user\n",
    "        response = requests.get(\n",
    "            f\"{RECOMMENDATION_SERVICE_URL}/recommendations\",\n",
    "            params={'userId': user_id, 'limit': 10, 'algorithm': 'hybrid'},\n",
    "            timeout=5\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            recommended_products = [rec['product_id'] for rec in data.get('recommendations', [])]\n",
    "            ground_truth_products = ground_truth.get(user_id, [])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if len(recommended_products) > 0 and len(ground_truth_products) > 0:\n",
    "                # Intersection of recommended and ground truth\n",
    "                intersection = set(recommended_products) & set(ground_truth_products)\n",
    "                \n",
    "                # Precision: relevant items retrieved / total items retrieved\n",
    "                precision = len(intersection) / len(recommended_products) if recommended_products else 0\n",
    "                \n",
    "                # Recall: relevant items retrieved / total relevant items\n",
    "                recall = len(intersection) / len(ground_truth_products) if ground_truth_products else 0\n",
    "                \n",
    "                # F1 Score\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                # Coverage: how many unique products recommended\n",
    "                coverage = len(set(recommended_products))\n",
    "                \n",
    "                accuracy_results.append({\n",
    "                    'user_id': user_id,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'coverage': coverage,\n",
    "                    'recommended_count': len(recommended_products),\n",
    "                    'ground_truth_count': len(ground_truth_products),\n",
    "                    'intersection_count': len(intersection)\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating accuracy for user {user_id}: {str(e)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "accuracy_df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "if len(accuracy_df) > 0:\n",
    "    # Calculate overall metrics\n",
    "    print(f\"\\nüìä Accuracy Metrics Summary:\")\n",
    "    print(f\"   Users tested: {len(accuracy_df)}\")\n",
    "    print(f\"   Average Precision: {accuracy_df['precision'].mean():.3f}\")\n",
    "    print(f\"   Average Recall: {accuracy_df['recall'].mean():.3f}\")\n",
    "    print(f\"   Average F1-Score: {accuracy_df['f1_score'].mean():.3f}\")\n",
    "    print(f\"   Average Coverage: {accuracy_df['coverage'].mean():.1f} products\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    print(f\"\\nüìà Metrics Distribution:\")\n",
    "    print(accuracy_df[['precision', 'recall', 'f1_score']].describe().round(3))\n",
    "    \n",
    "    # Visualize accuracy metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Precision distribution\n",
    "    accuracy_df['precision'].hist(bins=15, ax=axes[0], alpha=0.7, color='skyblue')\n",
    "    axes[0].axvline(accuracy_df['precision'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {accuracy_df[\"precision\"].mean():.3f}')\n",
    "    axes[0].set_title('Precision Distribution')\n",
    "    axes[0].set_xlabel('Precision')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Recall distribution\n",
    "    accuracy_df['recall'].hist(bins=15, ax=axes[1], alpha=0.7, color='lightgreen')\n",
    "    axes[1].axvline(accuracy_df['recall'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {accuracy_df[\"recall\"].mean():.3f}')\n",
    "    axes[1].set_title('Recall Distribution')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # F1-Score distribution\n",
    "    accuracy_df['f1_score'].hist(bins=15, ax=axes[2], alpha=0.7, color='lightcoral')\n",
    "    axes[2].axvline(accuracy_df['f1_score'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {accuracy_df[\"f1_score\"].mean():.3f}')\n",
    "    axes[2].set_title('F1-Score Distribution')\n",
    "    axes[2].set_xlabel('F1-Score')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(accuracy_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No accuracy data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc2f1d",
   "metadata": {},
   "source": [
    "## 8. Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "print(\"üìä Generating Comprehensive Evaluation Report...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Report timestamp\n",
    "report_time = datetime.now()\n",
    "print(f\"üìÖ Report Generated: {report_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Service Evaluated: Recommendation Engine (Port 8001)\")\n",
    "print(f\"‚è±Ô∏è Evaluation Duration: {(report_time - datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() / 3600:.2f} hours\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate overall scores\n",
    "if len(success_performance) > 0:\n",
    "    avg_response_time = success_performance['response_time_ms'].mean()\n",
    "    p95_response_time = success_performance['response_time_ms'].quantile(0.95)\n",
    "    cache_hit_rate = success_performance['cached'].mean()\n",
    "    success_rate = len(success_performance) / len(performance_df)\n",
    "    \n",
    "    print(f\"‚úÖ Overall Service Health: {'EXCELLENT' if success_rate > 0.95 else 'GOOD' if success_rate > 0.8 else 'NEEDS IMPROVEMENT'}\")\n",
    "    print(f\"‚ö° Performance Grade: {'A' if p95_response_time < 300 else 'B' if p95_response_time < 500 else 'C'}\")\n",
    "    print(f\"üíæ Cache Efficiency: {'HIGH' if cache_hit_rate > 0.8 else 'MEDIUM' if cache_hit_rate > 0.6 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ KEY PERFORMANCE INDICATORS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Performance KPIs\n",
    "if len(success_performance) > 0:\n",
    "    print(f\"üìä Response Time Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Average Response Time: {avg_response_time:.2f} ms\")\n",
    "    print(f\"   ‚Ä¢ Median Response Time: {success_performance['response_time_ms'].median():.2f} ms\")\n",
    "    print(f\"   ‚Ä¢ 95th Percentile: {p95_response_time:.2f} ms\")\n",
    "    print(f\"   ‚Ä¢ 99th Percentile: {success_performance['response_time_ms'].quantile(0.99):.2f} ms\")\n",
    "    \n",
    "    print(f\"\\nüíæ Cache Performance:\")\n",
    "    print(f\"   ‚Ä¢ Cache Hit Rate: {cache_hit_rate:.2%}\")\n",
    "    if cache_hit_rate > 0:\n",
    "        cached_avg = success_performance[success_performance['cached']]['response_time_ms'].mean()\n",
    "        uncached_avg = success_performance[~success_performance['cached']]['response_time_ms'].mean()\n",
    "        print(f\"   ‚Ä¢ Cached Response Time: {cached_avg:.2f} ms\")\n",
    "        print(f\"   ‚Ä¢ Uncached Response Time: {uncached_avg:.2f} ms\")\n",
    "        print(f\"   ‚Ä¢ Cache Speedup: {uncached_avg / cached_avg:.1f}x faster\")\n",
    "    \n",
    "    print(f\"\\nüéØ Service Reliability:\")\n",
    "    print(f\"   ‚Ä¢ Success Rate: {success_rate:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Total Requests: {len(performance_df)}\")\n",
    "    print(f\"   ‚Ä¢ Failed Requests: {len(performance_df) - len(success_performance)}\")\n",
    "\n",
    "# Tracking KPIs\n",
    "if len(tracking_df) > 0:\n",
    "    tracking_success_rate = tracking_df['success'].mean()\n",
    "    print(f\"\\nüìç Interaction Tracking:\")\n",
    "    print(f\"   ‚Ä¢ Tracking Success Rate: {tracking_success_rate:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Average Tracking Time: {tracking_df[tracking_df['success']]['response_time_ms'].mean():.2f} ms\")\n",
    "    print(f\"   ‚Ä¢ Total Interactions Tracked: {tracking_df['success'].sum()}\")\n",
    "\n",
    "# Accuracy KPIs\n",
    "if len(accuracy_df) > 0:\n",
    "    print(f\"\\nüéØ Recommendation Accuracy:\")\n",
    "    print(f\"   ‚Ä¢ Average Precision: {accuracy_df['precision'].mean():.3f}\")\n",
    "    print(f\"   ‚Ä¢ Average Recall: {accuracy_df['recall'].mean():.3f}\")\n",
    "    print(f\"   ‚Ä¢ Average F1-Score: {accuracy_df['f1_score'].mean():.3f}\")\n",
    "    print(f\"   ‚Ä¢ Average Coverage: {accuracy_df['coverage'].mean():.1f} products\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã STRATEGY PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(success_performance) > 0:\n",
    "    strategy_comparison = success_performance.groupby('strategy').agg({\n",
    "        'response_time_ms': ['mean', 'median'],\n",
    "        'cached': 'mean',\n",
    "        'personalized': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nStrategy Performance Summary:\")\n",
    "    print(strategy_comparison.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ GOALS ACHIEVEMENT ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define targets and assess\n",
    "targets = {\n",
    "    'response_time_p95': {'target': 500, 'actual': p95_response_time if len(success_performance) > 0 else 0, 'unit': 'ms'},\n",
    "    'cache_hit_rate': {'target': 0.80, 'actual': cache_hit_rate if len(success_performance) > 0 else 0, 'unit': '%'},\n",
    "    'success_rate': {'target': 0.95, 'actual': success_rate if len(success_performance) > 0 else 0, 'unit': '%'},\n",
    "    'tracking_success': {'target': 0.95, 'actual': tracking_success_rate if len(tracking_df) > 0 else 0, 'unit': '%'}\n",
    "}\n",
    "\n",
    "for metric, data in targets.items():\n",
    "    target = data['target']\n",
    "    actual = data['actual']\n",
    "    unit = data['unit']\n",
    "    \n",
    "    if unit == '%':\n",
    "        status = '‚úÖ PASS' if actual >= target else '‚ùå FAIL'\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {actual:.2%} (Target: {target:.0%}) {status}\")\n",
    "    else:\n",
    "        status = '‚úÖ PASS' if actual <= target else '‚ùå FAIL'\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {actual:.2f}{unit} (Target: <{target}{unit}) {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Performance recommendations\n",
    "if len(success_performance) > 0:\n",
    "    if p95_response_time > 500:\n",
    "        recommendations.append(\"‚ö° Optimize response time: P95 exceeds 500ms target\")\n",
    "    if cache_hit_rate < 0.80:\n",
    "        recommendations.append(\"üíæ Improve caching strategy: Cache hit rate below 80%\")\n",
    "    if success_rate < 0.95:\n",
    "        recommendations.append(\"üõ†Ô∏è Improve error handling: Success rate below 95%\")\n",
    "\n",
    "# Strategy-specific recommendations\n",
    "if len(success_performance) > 0:\n",
    "    slow_strategies = success_performance.groupby('strategy')['response_time_ms'].mean()\n",
    "    slowest_strategy = slow_strategies.idxmax()\n",
    "    if slow_strategies[slowest_strategy] > 300:\n",
    "        recommendations.append(f\"üîÑ Optimize '{slowest_strategy}' algorithm: Slowest performing strategy\")\n",
    "\n",
    "# General recommendations\n",
    "recommendations.extend([\n",
    "    \"üìä Implement A/B testing for different recommendation strategies\",\n",
    "    \"üîç Add more detailed user behavior tracking\",\n",
    "    \"üéØ Implement click-through rate measurement\",\n",
    "    \"üìà Set up automated performance monitoring\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ DATA EXPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    'performance_data': performance_df.to_dict('records') if len(performance_df) > 0 else [],\n",
    "    'tracking_data': tracking_df.to_dict('records') if len(tracking_df) > 0 else [],\n",
    "    'accuracy_data': accuracy_df.to_dict('records') if len(accuracy_df) > 0 else [],\n",
    "    'summary_metrics': {\n",
    "        'avg_response_time': avg_response_time if len(success_performance) > 0 else 0,\n",
    "        'p95_response_time': p95_response_time if len(success_performance) > 0 else 0,\n",
    "        'cache_hit_rate': cache_hit_rate if len(success_performance) > 0 else 0,\n",
    "        'success_rate': success_rate if len(success_performance) > 0 else 0,\n",
    "        'total_requests': len(performance_df),\n",
    "        'evaluation_timestamp': report_time.isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "import os\n",
    "os.makedirs('d:/Edu/graduation-project/report/evaluation/results', exist_ok=True)\n",
    "results_file = f'd:/Edu/graduation-project/report/evaluation/results/recommendation_evaluation_{report_time.strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìÅ Evaluation results saved to: {results_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Total Test Cases: {len(performance_df) + len(tracking_df)}\")\n",
    "print(f\"‚è±Ô∏è Evaluation Time: {report_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"üíæ Results Saved: {results_file}\")\n",
    "print(\"‚úÖ Ready for production deployment assessment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
