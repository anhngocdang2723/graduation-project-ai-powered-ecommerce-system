# ğŸ“ ÄIá»‚M Ná»”I Báº¬T VÃ€ CÃ”NG NGHá»† CHI TIáº¾T

## ğŸ“Œ I. ÄIá»‚M Ná»”I Báº¬T Cá»¦A CHATBOT MULTI-AGENT

### 1.1 Táº¡i Sao Cáº§n Multi-Agent Architecture?

**Váº¥n Ä‘á» vá»›i LLM-only approach:**
- âŒ Cháº­m: LLM inference máº¥t 1-3 giÃ¢y
- âŒ Tá»‘n kÃ©m: $0.01+ per API call
- âŒ KhÃ´ng reliable: Hallucination, sai tool call
- âŒ KhÃ´ng scalable: Rate limiting tá»« LLM provider

**Giáº£i phÃ¡p: Multi-Agent Architecture**
- âœ… Nhanh: 50-100ms cho háº§u háº¿t requests
- âœ… Ráº»: $0 cho 90% requests (NLP-only)
- âœ… Reliable: Rule-based + explicit tool calls
- âœ… Scalable: Horizontal scaling khÃ´ng bá»‹ rate limit

---

### 1.2 5 Agents Giáº£i Quyáº¿t Nhá»¯ng Váº¥n Äá» GÃ¬?

```
PROBLEM: "TÃ´i muá»‘n tÃ¬m balo mÃ u Ä‘á»"

WITHOUT Multi-Agent:
â””â”€> Single LLM
    â”œâ”€ Parse user intent (expensive)
    â”œâ”€ Extract entities (expensive)
    â”œâ”€ Choose tool (error-prone)
    â”œâ”€ Call tool (expensive)
    â”œâ”€ Generate response (expensive)
    â””â”€ Response time: 2-3 seconds, Cost: $0.01

WITH Multi-Agent:
â””â”€> Agent 1: Clean text (50ms, $0)
    â””â”€> Agent 2: Classify intent = PRODUCT.SEARCH (20ms, $0)
        â””â”€> Agent 3: Validate permissions (10ms, $0)
            â””â”€> Agent 4: Execute search_products("balo mÃ u Ä‘á»") (150ms, $0)
                â””â”€> Agent 5: Generate response from template (30ms, $0)
                    â””â”€ Response time: 260ms, Cost: $0
```

---

### 1.3 Decision Tree vs LLM

**Decision Tree Approach (Agent 2):**

```
User: "tÃ¬m balo"
â”‚
â”œâ”€ Contains "tÃ¬m" OR "search"? â†’ YES
â”‚
â”œâ”€ Contains "order" OR "cart"? â†’ NO
â”‚
â”œâ”€ Contains "sáº£n pháº©m" OR "product"? â†’ YES
â”‚
â”œâ”€ Contains "so sÃ¡nh" OR "compare"? â†’ NO
â”‚
â””â”€> INTENT = PRODUCT.SEARCH âœ…
    Extract query: "balo"
```

**LLM Approach:**

```
User: "tÃ¬m balo"
â”‚
â””â”€> Call LLM: "What is the user's intent?"
    â””â”€> LLM processes entire context
        â””â”€> Returns: {"intent": "search_product", "query": "balo"}
            â””â”€> 1000-3000ms, $0.001+, possible hallucination
```

**Comparison:**
| Aspect | Decision Tree | LLM |
|--------|--------------|-----|
| Speed | 10-20ms | 1000-3000ms |
| Cost | $0 | $0.001+ |
| Accuracy | 95%+ | 85-90% |
| Edge cases | Limited | Flexible |
| Interpretable | âœ… Yes | âŒ Black box |

---

### 1.4 Tool Execution Pattern

**Pattern 1: Single Tool Call**

```
Intent: PRODUCT.SEARCH
Query: "balo Ä‘á»"
â”‚
â””â”€> Executor calls:
    â””â”€> search_products(query="balo Ä‘á»", limit=5)
        â””â”€> Returns: [prod_1, prod_2, prod_3]
            â””â”€> Response Generator creates template:
                "TÃ¬m tháº¥y 3 sáº£n pháº©m cho 'balo Ä‘á»'"
```

**Pattern 2: Multi-Step Conversation**

```
Step 1 - User: "Theo dÃµi Ä‘Æ¡n hÃ ng"
         Intent: ORDER.TRACK (requires order_id)
         Response: "Vui lÃ²ng cung cáº¥p sá»‘ Ä‘Æ¡n hÃ ng"

Step 2 - User: "123456"
         Context: order_id = "123456"
         Executor calls:
         â””â”€> get_order(order_id="123456")
             â””â”€> Returns: {status: "shipped", tracking: "..."}
                 â””â”€> Response: "ÄÆ¡n hÃ ng #123456 Ä‘ang váº­n chuyá»ƒn..."
```

**Pattern 3: Permission-Based Routing**

```
User: "guest" (chÆ°a Ä‘Äƒng nháº­p)
Intent: ORDER.TRACK
â”‚
â”œâ”€ Permission check: customer-only intent?
â”‚
â””â”€> NO PERMISSION
    Response: "Vui lÃ²ng Ä‘Äƒng nháº­p Ä‘á»ƒ xem Ä‘Æ¡n hÃ ng"
    Action: show_login_modal
```

---

### 1.5 Human Escalation Flow

```
SCENARIO: Customer says "TÃ´i muá»‘n nÃ³i chuyá»‡n vá»›i nhÃ¢n viÃªn"

Step 1 - Detect escalation
â””â”€> Intent = SUPPORT.ESCALATE

Step 2 - Orchestrator validates
â””â”€> Can escalate? Yes
    â””â”€> session.status = 'escalated'

Step 3 - Notify admin dashboard
â””â”€> WebSocket message:
    {
      "type": "escalation_request",
      "session_id": "sess_12345",
      "customer": "john@email.com",
      "last_message": "Sáº£n pháº©m bá»‹ há»ng, tÃ´i muá»‘n Ä‘á»•i",
      "conversation_history": [...]
    }

Step 4 - Admin UI
â””â”€> ğŸ”” Notification appears
    â”œâ”€ Show customer info
    â”œâ”€ Show conversation history
    â””â”€ [Take Over] button

Step 5 - Staff takes over
â””â”€> Click "Take Over"
    â””â”€> session.staff_id = 'staff_123'
    â””â”€> AI pauses, chat continues with human

Step 6 - Customer receives human support
â””â”€> Customer & staff chat in real-time
    â””â”€> AI monitors (can suggest quick replies)
        â””â”€> Staff resolves issue
            â””â”€> Close conversation
```

---

### 1.6 Error Handling & Resilience

**Example: Product Search Fails**

```
User: "tÃ¬m balo Ä‘á»"
â”‚
â””â”€> Executor calls: search_products(query="balo Ä‘á»")
    â””â”€> API timeout / error
        â”‚
        â”œâ”€ Retry logic:
        â”‚  â””â”€ Retry 1: Wait 1s, try again
        â”‚     Fail again
        â”‚  â””â”€ Retry 2: Wait 2s, try again
        â”‚     Fail again
        â”‚  â””â”€ Max retries reached
        â”‚
        â””â”€ Fallback:
           â”œâ”€ Check cache for recent search results
           â”œâ”€ Return cached results if available
           â”œâ”€ Or return empty but helpful response:
           â”‚  "Hiá»‡n táº¡i khÃ´ng thá»ƒ tÃ¬m kiáº¿m, vui lÃ²ng thá»­ láº¡i"
           â””â”€ Log error for monitoring
```

---

## ğŸ“Š II. ÄIá»‚M Ná»”I Báº¬T Cá»¦A RECOMMENDATION ENGINE

### 2.1 Hybrid Algorithm vs Pure Approaches

**Problem:** Chá»n phÆ°Æ¡ng phÃ¡p nÃ o cho ML recommendation?

**Approaches:**

```
1. CONTENT-BASED ONLY
   â”œâ”€ Pro: Simple, interpretable, no cold start
   â”œâ”€ Con: Lack of discovery (only similar to viewed)
   â””â”€ Result: Limited diversity, boring recommendations

2. COLLABORATIVE FILTERING ONLY
   â”œâ”€ Pro: Discover new products, proven in Netflix/Amazon
   â”œâ”€ Con: Cold start problem (new users/products)
   â””â”€ Result: Good for active users, bad for new users

3. HYBRID (OUR CHOICE) â­
   â”œâ”€ Pro: Combines strength of both
   â”‚  â”œâ”€ Content: Handles new users/products
   â”‚  â”œâ”€ Collaborative: Discovers new items
   â”‚  â””â”€ Both: Better accuracy
   â”‚
   â””â”€ Formula: Score = 0.4Ã—Content + 0.6Ã—Collaborative
```

**Formula Explanation:**

```
HYBRID_SCORE(user, product) = 
    0.4 Ã— CONTENT_SCORE(user, product) +
    0.6 Ã— COLLABORATIVE_SCORE(user, product)

WHERE:
  CONTENT_SCORE = 
    Î£(user_category_preference[cat] Ã— product_category_match[cat])
    â””â”€ How much user likes this product's category?
       Example: If user viewed 5 backpacks, score=0.8
                If user never viewed shoes, score=0.2

  COLLABORATIVE_SCORE = 
    Î£(similarity(user, user_i) Ã— rating(user_i, product))
    â””â”€ Would similar users buy this product?
       Example: 10 similar users bought this product â†’ score=0.7
```

---

### 2.2 5 Recommendation Strategies

**Strategy 1: Hybrid (Default) - 70% of requests**

```
Use case: Regular users with some history
Score = 0.4Ã—Content + 0.6Ã—Collaborative
Time: 200-500ms (computed) / <50ms (cached)
```

**Strategy 2: Content-Based - 10% of requests**

```
Use case: New users (cold start problem)
How: "Show products similar to what you viewed"
Products: Same category as viewed items
Time: <100ms
```

**Strategy 3: Collaborative Filtering - 10% of requests**

```
Use case: Mature users with lots of interactions
How: "Show products bought by similar users"
Products: What users like you also purchased
Time: 300-500ms
```

**Strategy 4: Trending - 5% of requests**

```
Use case: Discover what's popular
Metric: Most viewed in last 7 days
Time: <50ms (cached)
```

**Strategy 5: Frequently Bought Together - 5% of requests**

```
Use case: Cross-sell / upsell
How: "Often bought together"
Products: Products frequently co-purchased
Time: <50ms (pre-computed)
```

---

### 2.3 Cold Start Problem Solution

**Problem:** New user arrives â†’ No interaction history â†’ Can't recommend

**Solution:**

```
New User Flow:

1. First page view: Product A
   â””â”€ Track as interaction (content_based score)
   â””â”€ Recommendation strategy = Content-Based
       â””â”€ Find products in same category
       â””â”€ Recommend top 5

2. Second page view: Product B
   â””â”€ Now has 2 interactions
   â””â”€ Still use Content-Based (not enough data)

3. After ~10 interactions
   â””â”€ User has enough history
   â””â”€ Switch to Hybrid algorithm
   â””â”€ Better recommendations

4. Algorithm selection:
   if interaction_count < 5:
     use ContentBased()
   elif interaction_count < 20:
     use Hybrid(w_content=0.6, w_collab=0.4)
   else:
     use Hybrid(w_content=0.4, w_collab=0.6)
```

---

### 2.4 Caching Strategy

**Why Cache?**

```
Without cache:
- User loads homepage
- Need recommendations for 3 products
- Each needs computation: 200-500ms
- Total: 600-1500ms âŒ Too slow!

With cache:
- First time: Compute + store in Redis â†’ 500ms
- Next 50 users (within 1 hour TTL): <50ms âœ… Fast!
```

**Cache Strategy:**

```python
def get_recommendations(user_id, limit=10):
    # Step 1: Check Redis cache
    cache_key = f"rec:recommendations:{user_id}"
    cached = redis.get(cache_key)
    
    if cached and not expired(cached):
        return cached  # <50ms âœ…
    
    # Step 2: Compute recommendations
    recommendations = compute_hybrid_score(user_id, limit)
    
    # Step 3: Store in cache (1 hour TTL)
    redis.setex(
        cache_key,
        3600,  # 1 hour
        json.dumps(recommendations)
    )
    
    return recommendations  # First time 200-500ms
```

**Cache Hit Rate Analysis:**

```
Assumption: 1000 users/day

Scenario 1 - Low engagement:
â””â”€ Users visit 1-2 times/day
â””â”€ Cache TTL: 1 hour
â””â”€ Cache hit rate: ~80%
â””â”€ Avg response: 0.8Ã—50ms + 0.2Ã—300ms = 100ms

Scenario 2 - High engagement:
â””â”€ Users visit 5-10 times/day
â””â”€ Same TTL
â””â”€ Cache hit rate: ~95%
â””â”€ Avg response: 0.95Ã—50ms + 0.05Ã—300ms = 65ms

Target: >80% cache hit rate
```

---

### 2.5 Similarity Computation

**Product Similarity (Pre-computed Nightly)**

```python
def compute_product_similarities():
    """
    For each product pair, calculate similarity score.
    Run once/day at 2 AM (off-peak).
    Result: Stored in rec_product_similarities table
    """
    
    # Algorithm:
    products = get_all_products()  # 100+ products
    
    for i in range(len(products)):
        for j in range(i+1, len(products)):
            prod_i = products[i]
            prod_j = products[j]
            
            # Calculate similarity
            similarity = 0
            
            # 1. Category match (60% weight)
            if prod_i.category == prod_j.category:
                similarity += 0.6
            
            # 2. Tag overlap (20% weight)
            tag_overlap = len(set(prod_i.tags) & set(prod_j.tags))
            similarity += 0.2 * (tag_overlap / max_tags)
            
            # 3. Co-occurrence (20% weight)
            # How often bought together?
            co_buys = count_co_purchases(prod_i.id, prod_j.id)
            similarity += 0.2 * min(co_buys / 10, 1.0)
            
            # Store result
            store_similarity(prod_i.id, prod_j.id, similarity)
    
    # Cost: O(nÂ²) for n products
    # Optimization: Only update if products changed
```

**User Similarity (For Collaborative Filtering)**

```python
def compute_user_similarities():
    """
    Find similar users based on interaction patterns.
    Used during recommendation generation.
    """
    
    # Algorithm: Cosine similarity on preference vectors
    
    users = get_all_users()
    
    for user in users:
        # Get user's preference vector
        prefs = get_user_preferences(user.id)
        # prefs = {backpack: 0.9, shoes: 0.3, jacket: 0.2}
        
        # Find k nearest neighbors (top 10 similar users)
        similar_users = []
        
        for other_user in users:
            if other_user.id == user.id:
                continue
            
            other_prefs = get_user_preferences(other_user.id)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(prefs, other_prefs)
            
            similar_users.append((other_user.id, similarity))
        
        # Sort and keep top 10
        similar_users = sorted(similar_users, 
                              key=lambda x: x[1], 
                              reverse=True)[:10]
        
        # Store for later use in recommendations
        store_similar_users(user.id, similar_users)
```

---

## ğŸ”„ III. INTEGRATION POINTS

### 3.1 Frontend â†’ Recommendation Service

```
USER ACTION: Views product page
â”‚
â”œâ”€ Frontend logs: trackProductView(productId)
â”‚  â””â”€ POST /api/recommendations/track
â”‚     {
â”‚       "user_id": "guest_uuid_or_customer_id",
â”‚       "session_id": "session_abc123",
â”‚       "product_id": "prod_jansport_superbreak",
â”‚       "interaction_type": "view",
â”‚       "metadata": {
â”‚         "category": "backpack",
â”‚         "price": 1299000,
â”‚         "title": "JanSport Superbreak"
â”‚       }
â”‚     }
â”‚
â”œâ”€ Next.js API route proxies to Recommendation Service
â”‚  â””â”€ HTTP POST http://recommendation:8001/track
â”‚
â”œâ”€ Recommendation Service processes
â”‚  â””â”€ INSERT into rec_user_interactions
â”‚  â””â”€ UPDATE rec_user_preferences (learn preferences)
â”‚
â””â”€ Response: {"success": true, "interaction_id": "..."}

---

USER ACTION: Views homepage
â”‚
â”œâ”€ Frontend loads: <PersonalizedRecommendations />
â”‚  â””â”€ GET /api/recommendations?userId=XXX&limit=10
â”‚
â”œâ”€ Next.js API route proxies
â”‚  â””â”€ HTTP GET http://recommendation:8001/recommendations?userId=XXX&limit=10
â”‚
â”œâ”€ Recommendation Service executes
â”‚  â”œâ”€ Check Redis cache (hit = <50ms)
â”‚  â”œâ”€ Or compute hybrid score
â”‚  â”‚  â”œâ”€ Read rec_user_interactions (last 30 days)
â”‚  â”‚  â”œâ”€ Read rec_user_preferences
â”‚  â”‚  â”œâ”€ Query rec_product_similarities
â”‚  â”‚  â”œâ”€ Calculate: Score = 0.4Ã—content + 0.6Ã—collab
â”‚  â”‚  â”œâ”€ Sort products by score
â”‚  â”‚  â””â”€ Cache result in Redis (1 hour TTL)
â”‚  â”‚
â”‚  â””â”€ Return: [{product_id, score, reason}, ...]
â”‚
â””â”€ Frontend displays products with score-based ordering
```

---

### 3.2 Frontend â†’ Chatbot Service

```
USER ACTION: Types message in chat widget
â”‚
â”œâ”€ Frontend sends: ChatMessage (WebSocket)
â”‚  â””â”€ WS POST ws://localhost:8000/ws/chat
â”‚     {
â”‚       "message": "tÃ¬m balo mÃ u Ä‘á»",
â”‚       "user_id": "user_123",
â”‚       "session_id": "session_456"
â”‚     }
â”‚
â”œâ”€ WebSocket connection to Chatbot Service
â”‚
â”œâ”€ Chatbot Pipeline processes (5 agents)
â”‚  â”œâ”€ Agent 1: Clean text â†’ "tÃ¬m balo mÃ u Ä‘á»"
â”‚  â”œâ”€ Agent 2: Classify intent â†’ PRODUCT.SEARCH
â”‚  â”œâ”€ Agent 3: Validate â†’ OK
â”‚  â”œâ”€ Agent 4: Execute tool â†’ search_products("balo Ä‘á»")
â”‚  â”‚  â””â”€ Query Medusa API: GET /store/products?q=balo
â”‚  â”œâ”€ Agent 5: Generate response â†’ Template + Product list
â”‚  â”‚
â”‚  â””â”€ Response:
â”‚     {
â”‚       "response": "TÃ¬m tháº¥y 3 sáº£n pháº©m cho 'balo Ä‘á»'",
â”‚       "products": [
â”‚         {
â”‚           "id": "prod_123",
â”‚           "title": "JanSport Superbreak Red",
â”‚           "price": 1299000,
â”‚           "image": "url"
â”‚         }
â”‚       ],
â”‚       "quick_replies": ["Xem chi tiáº¿t", "ThÃªm vÃ o giá»"],
â”‚       "actions": ["show_products"]
â”‚     }
â”‚
â””â”€ Frontend displays message + product cards in widget
```

---

### 3.3 Chatbot Service â†’ Medusa Backend

```
TOOL CALL: search_products("balo")
â”‚
â”œâ”€ Chatbot Executor calls Medusa API
â”‚  â””â”€ GET http://medusa:9000/store/products?q=balo
â”‚
â”œâ”€ Medusa processes search
â”‚  â”œâ”€ Query: SELECT * FROM product WHERE title LIKE '%balo%'
â”‚  â”œâ”€ Apply filters, sorting
â”‚  â””â”€ Return: [{id, title, price, ...}, ...]
â”‚
â””â”€ Response: [prod_1, prod_2, prod_3]
   â””â”€ Chatbot extracts key info â†’ Response
      â””â”€ Frontend displays with prices, images
```

---

## ğŸ›¡ï¸ IV. ERROR HANDLING & RESILIENCE

### 4.1 Chatbot Error Handling

```
Failure Scenario 1: Medusa API Down
â”œâ”€ Tool call: search_products() fails
â”œâ”€ Retry logic: 3 retries with exponential backoff
â”œâ”€ Backoff: 1s, 2s, 4s
â”œâ”€ Still fails?
â”œâ”€ Response fallback: "Hiá»‡n táº¡i khÃ´ng thá»ƒ tÃ¬m kiáº¿m, vui lÃ²ng thá»­ láº¡i"
â””â”€ Log error for debugging

Failure Scenario 2: Database timeout
â”œâ”€ Query hangs (network issue)
â”œâ”€ Timeout after 5s
â”œâ”€ Retry with shorter timeout
â”œâ”€ Still slow? Return cached results if available
â””â”€ User sees old data but no broken experience

Failure Scenario 3: LLM API down (Qwen3)
â”œâ”€ Fallback LLM needed
â”œâ”€ Response fallback: Use default template
â”œâ”€ No personalized response, but works
â””â”€ Escalate to human if needed

Failure Scenario 4: Invalid user input
â”œâ”€ Input validation at Agent 1
â”œâ”€ Sanitize text: Remove special chars
â”œâ”€ Max length: 500 chars
â”œâ”€ If invalid: "KhÃ´ng hiá»ƒu yÃªu cáº§u, vui lÃ²ng thá»­ láº¡i"
â””â”€ Don't pass to later agents
```

---

### 4.2 Recommendation Error Handling

```
Failure Scenario 1: Redis cache fails
â”œâ”€ Fall back to direct database computation
â”œâ”€ Slower (200-500ms vs <50ms) but works
â””â”€ Monitor cache health

Failure Scenario 2: Product similarity not computed
â”œâ”€ Use fallback algorithm (pure content-based)
â”œâ”€ Similarity computation job might have failed
â”œâ”€ Monitor batch job logs
â””â”€ Trigger manual recompute

Failure Scenario 3: No user history (new user)
â”œâ”€ Use trending products (most viewed)
â”œâ”€ Or featured collection
â”œâ”€ Switch to content-based as user interacts
â””â”€ Handle cold start gracefully

Failure Scenario 4: Concurrent recommendation requests
â”œâ”€ Rate limiting: 10 requests/second per user
â”œâ”€ Queue excess requests
â”œâ”€ Serve from cache if available
â””â”€ Return previous recommendations if timeout
```

---

## ğŸ“ˆ V. PERFORMANCE OPTIMIZATION

### 5.1 Database Query Optimization

```sql
-- âŒ SLOW: Full table scan
SELECT * FROM rec_user_interactions 
WHERE user_id = 'user_123';
-- Cost: O(n) where n = total rows

-- âœ… FAST: Indexed query
CREATE INDEX idx_user_id ON rec_user_interactions(user_id);
CREATE INDEX idx_product_id ON rec_user_interactions(product_id);

SELECT * FROM rec_user_interactions 
WHERE user_id = 'user_123'
ORDER BY created_at DESC 
LIMIT 10;
-- Cost: O(log n) or O(1) with index

-- âœ… FASTER: Partition by date (for large tables)
CREATE TABLE rec_user_interactions_2025_12 
PARTITION OF rec_user_interactions 
FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');
-- Cost: O(1) for recent data
```

---

### 5.2 Algorithm Optimization

```python
# âŒ SLOW: Compute similarity for all product pairs
def compute_similarities_slow():
    products = get_all_products()  # 100+
    for i in range(len(products)):
        for j in range(i+1, len(products)):
            similarity = calculate(products[i], products[j])
            # Time: O(nÂ²) â‰ˆ 5000 computations!

# âœ… FAST: Only compute for candidate products
def compute_similarities_fast():
    products = get_all_products()
    for product in products:
        # Only find similar products (same category)
        candidates = get_products(category=product.category)
        # Time: O(n * m) where m << n

# âœ… FASTER: Pre-compute & cache
def compute_similarities_cached():
    # Step 1: Pre-compute nightly (off-peak)
    # Step 2: Cache in rec_product_similarities
    # Step 3: During day, just lookup
    # Time: O(1) for each recommendation
```

---

### 5.3 Frontend Performance

```
OPTIMIZATION: Lazy load recommendations

âŒ Load all recommendations on page load
â”œâ”€ Blocks page render
â””â”€ User waits 1-2 seconds

âœ… Use React Suspense + Server Components
â”œâ”€ Initial load: Show skeleton
â”œâ”€ Stream recommendations as ready
â”œâ”€ User sees content immediately
â””â”€ Recommendations fill in as data ready

âœ… Intersection Observer (client-side)
â”œâ”€ Only load recommendations when visible
â”œâ”€ If below fold, load when scrolled to
â””â”€ Reduces initial load time
```

---

## ğŸ¯ VI. TESTING & MONITORING

### 6.1 Unit Testing Strategy

```
Chatbot:
â”œâ”€ test_input_processor.py
â”‚  â”œâ”€ Test text normalization
â”‚  â”œâ”€ Test language detection
â”‚  â””â”€ Test spell check
â”œâ”€ test_intent_classifier.py
â”‚  â”œâ”€ Test keyword matching
â”‚  â”œâ”€ Test decision tree
â”‚  â””â”€ Test entity extraction
â”œâ”€ test_executor.py
â”‚  â”œâ”€ Mock Medusa API
â”‚  â”œâ”€ Test tool calls
â”‚  â””â”€ Test error handling
â””â”€ test_response_generator.py
   â”œâ”€ Test template rendering
   â”œâ”€ Test LLM fallback
   â””â”€ Test response structure

Recommendation:
â”œâ”€ test_hybrid_algorithm.py
â”‚  â”œâ”€ Test scoring formula
â”‚  â”œâ”€ Test weight calculation
â”‚  â””â”€ Test ranking
â”œâ”€ test_similarity.py
â”‚  â”œâ”€ Test cosine similarity
â”‚  â”œâ”€ Test category matching
â”‚  â””â”€ Test co-occurrence
â””â”€ test_cache.py
   â”œâ”€ Test Redis operations
   â”œâ”€ Test TTL expiry
   â””â”€ Test cache invalidation
```

### 6.2 Integration Testing

```bash
# Chatbot service
python -m pytest chatbot-service/tests/test_chat_pipeline.py

# Recommendation service
python -m pytest recommendation-service/tests/test_quick.py

# E2E tests
pytest tests/e2e/test_homepage_personalization.py
pytest tests/e2e/test_chatbot_workflow.py
```

### 6.3 Monitoring Metrics

```
Chatbot Metrics (track in database):
â”œâ”€ Intent classification accuracy
â”œâ”€ Tool execution success rate
â”œâ”€ Response time distribution (p50, p95, p99)
â”œâ”€ LLM fallback frequency
â”œâ”€ Escalation rate
â””â”€ User satisfaction (if survey)

Recommendation Metrics:
â”œâ”€ Click-through rate (CTR)
â”œâ”€ Conversion rate
â”œâ”€ Cache hit rate
â”œâ”€ Average recommendation latency
â”œâ”€ Recommendation diversity
â””â”€ Cold start handling effectiveness

System Metrics:
â”œâ”€ API latency
â”œâ”€ Error rates
â”œâ”€ Database query times
â”œâ”€ Cache efficiency
â””â”€ Resource utilization (CPU, memory, disk)
```

---

## ğŸ“ VII. LEARNING & FUTURE IMPROVEMENTS

### 7.1 What This Project Demonstrates

âœ… **Software Architecture:**
- Microservices design
- API design & integration
- Database normalization
- Caching strategies
- Error handling & resilience

âœ… **AI/ML:**
- Multi-agent systems
- Intent classification
- Hybrid recommendation algorithms
- Collaborative filtering
- Cold start problem handling

âœ… **DevOps:**
- Docker & containerization
- Docker Compose orchestration
- Service discovery
- Environment management
- Scaling considerations

âœ… **Full-Stack Development:**
- Frontend (Next.js, React)
- Backend (Node.js, Python)
- Database (PostgreSQL)
- Caching (Redis)
- Real-time (WebSocket)

---

### 7.2 Key Learnings

1. **Don't use LLM for everything**
   - Rule-based NLP can handle 90% of cases
   - LLM should be fallback, not default
   - Save money by being smart about when to use LLM

2. **Hybrid approaches beat single methods**
   - Hybrid recommendation > pure content or collaborative
   - Multi-agent > monolithic system
   - Combine rule-based + ML for robustness

3. **Caching is critical**
   - Cache hit rate 80%+ makes huge difference
   - Redis is simple and effective
   - Know your cache invalidation strategy

4. **User experience matters**
   - 200ms response time feels fast
   - 1000ms+ feels slow
   - Optimize for p95 latency, not just average

5. **Monitoring is essential**
   - Track metrics from day 1
   - Know your bottlenecks
   - Data-driven optimization

---

## ğŸ“š VIII. REFERENCES & RESOURCES

### 8.1 Architecture References

- [System Design Interview Book](https://www.educative.io/courses/grokking-the-system-design-interview)
- [Recommendation System Design](https://developers.google.com/machine-learning/recommendation)
- [Building Microservices](https://microservices.io/)

### 8.2 Technology Documentation

- [FastAPI](https://fastapi.tiangolo.com/) - Python web framework
- [Next.js](https://nextjs.org/docs) - React framework
- [Medusa](https://docs.medusajs.com/) - E-commerce platform
- [PostgreSQL](https://www.postgresql.org/docs/) - Database
- [Redis](https://redis.io/documentation) - Cache
- [scikit-learn](https://scikit-learn.org/stable/) - ML library

### 8.3 Papers & Articles

- [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)
- [Content-Based Filtering](https://en.wikipedia.org/wiki/Content-based_filtering)
- [Hybrid Recommender Systems](https://en.wikipedia.org/wiki/Recommender_system#Hybrid_recommender_systems)

---

**Document Version:** v1.0  
**Last Updated:** December 15, 2025  
**Status:** Production-Ready âœ…
