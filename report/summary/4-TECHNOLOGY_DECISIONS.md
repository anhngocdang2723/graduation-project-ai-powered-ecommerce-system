# ğŸ”§ SO SÃNH CÃ”NG NGHá»† & Lá»°A CHá»ŒN THIáº¾T Káº¾

## ğŸ“Œ I. Lá»°A CHá»ŒN KIáº¾N TRÃšC

### 1.1 Monolithic vs Microservices

**Monolithic Approach (âŒ Not Used):**

```
Single Large Application
â”œâ”€â”€ Frontend (React)
â”œâ”€â”€ E-commerce API (Node.js)
â”œâ”€â”€ Chatbot AI (Python)
â”œâ”€â”€ Recommendation Engine (Python)
â””â”€â”€ All in one process/container

Pros:
âœ… Simpler to develop initially
âœ… Easier to debug
âœ… Fewer network calls
âœ… Single database transaction

Cons:
âŒ Hard to scale individual components
âŒ One failure brings down everything
âŒ Technology mixing (Node + Python same process impossible)
âŒ Difficult to deploy independently
âŒ Slower iteration on one feature
```

**Microservices Approach (âœ… Our Choice):**

```
5 Independent Services
â”œâ”€â”€ Frontend (Next.js) â†’ Port 3000
â”œâ”€â”€ Medusa Backend (Node.js) â†’ Port 9000
â”œâ”€â”€ Chatbot Service (FastAPI) â†’ Port 8000
â”œâ”€â”€ Recommendation Service (FastAPI) â†’ Port 8001
â””â”€â”€ Shared: PostgreSQL, Redis

Pros:
âœ… Scales independently (e.g., more chatbot replicas)
âœ… Fault isolation (Chatbot down â‰  store down)
âœ… Technology choice per service
âœ… Deploy independently
âœ… Easier testing
âœ… Easier for team division

Cons:
âŒ More complex (multiple deployments)
âŒ Network latency between services
âŒ Data consistency challenges
âŒ Requires good monitoring

Decision: Microservices is RIGHT for this project
â”œâ”€â”€ Each service has different scaling needs
â”œâ”€â”€ Different tech stacks (Node + Python)
â””â”€â”€ Clear separation of concerns
```

---

### 1.2 Frontend Framework Choice

**Comparison: React SPA vs Next.js vs Remix**

```
REACT SPA (Create React App)
â”œâ”€â”€ Traditional: Client renders everything
â”œâ”€â”€ âŒ Pros:
â”‚  â”œâ”€ Rich interactivity
â”‚  â””â”€ Offline capability (with service workers)
â”œâ”€â”€ âŒ Cons:
â”‚  â”œâ”€ Poor SEO (JS rendered content)
â”‚  â”œâ”€ Slow initial load
â”‚  â”œâ”€ More client-side bundle
â”‚  â””â”€ Complexity for tracking (client-side only)
â””â”€ Not suitable: E-commerce needs SEO

NEXT.JS 14 (âœ… OUR CHOICE)
â”œâ”€â”€ Server-side rendering + Client components
â”œâ”€â”€ âœ… Pros:
â”‚  â”œâ”€ Great SEO (server renders HTML)
â”‚  â”œâ”€ Fast initial load (streaming)
â”‚  â”œâ”€ React Server Components (RSC)
â”‚  â”œâ”€ Built-in optimization
â”‚  â”œâ”€ API routes (proxy to backend)
â”‚  â”œâ”€ Image optimization
â”‚  â””â”€ Vercel deployment ready
â”œâ”€â”€ âœ… Cons:
â”‚  â”œâ”€ More server resources
â”‚  â””â”€ Learning curve (RSC, streaming)
â””â”€ Perfect for: E-commerce + Personalization

REMIX
â”œâ”€â”€ Similar to Next.js
â”œâ”€â”€ Focus on: Form handling, progressive enhancement
â”œâ”€â”€ Not chosen: Next.js more mature ecosystem
```

**Why Next.js 14 for our project:**

```
Requirements:
1. SEO (product pages need to rank)
2. Performance (personalization = no single HTML)
3. Tracking (need to integrate with recommendation API)
4. Conversions (fast load = higher conversion)

Next.js Features Used:
â”œâ”€â”€ Server Components: Layout, static pages
â”œâ”€â”€ Client Components: Interactivity (cart, wishlist)
â”œâ”€â”€ Server Actions: Form submissions
â”œâ”€â”€ Image Optimization: <Image> component
â”œâ”€â”€ API Routes: Proxy to Medusa, Recommendation, Chatbot
â”œâ”€â”€ Streaming: Suspense boundaries
â””â”€â”€ Edge Runtime: Optional (for CDN)

Result: Fast, SEO-friendly, easy integration
```

---

## ğŸ¤– II. Lá»°A CHá»ŒN CHATBOT ARCHITECTURE

### 2.1 Comparison: LLM-Only vs Rule-Based vs Hybrid

```
APPROACH 1: LLM-ONLY (Pure AI)
â”œâ”€â”€ Architecture: User â†’ LLM â†’ Tool Call â†’ Response
â”œâ”€â”€ âŒ Pros:
â”‚  â”œâ”€ Very flexible
â”‚  â”œâ”€ Handles edge cases
â”‚  â””â”€ No rule maintenance
â”œâ”€â”€ âŒ Cons:
â”‚  â”œâ”€ Slow: 1-3 seconds per request
â”‚  â”œâ”€ Expensive: $0.001-0.01 per call
â”‚  â”œâ”€ Unreliable: Hallucinations, wrong tools
â”‚  â”œâ”€ Rate limited: LLM provider limits
â”‚  â””â”€ Not scalable
â”œâ”€â”€ Example: OpenAI Assistant API
â””â”€â”€ Cost/month: ~$100-500 for 1000 users

APPROACH 2: RULE-BASED ONLY (Pure Logic)
â”œâ”€â”€ Architecture: User â†’ Keyword Match â†’ Rule â†’ Response
â”œâ”€â”€ âœ… Pros:
â”‚  â”œâ”€ Fast: <100ms
â”‚  â”œâ”€ Cheap: $0 (no API calls)
â”‚  â”œâ”€ Reliable: Predictable
â”‚  â”œâ”€ Scalable: Horizontal easily
â”‚  â””â”€ Interpretable: Easy to debug
â”œâ”€â”€ âŒ Cons:
â”‚  â”œâ”€ Low flexibility
â”‚  â”œâ”€ Hard to maintain (rules grow)
â”‚  â”œâ”€ Edge cases not handled
â”‚  â””â”€ Limited to predetermined intents
â”œâ”€â”€ Example: Old chatbots, menu-driven systems
â””â”€â”€ Cost/month: $0 (but engineering time)

APPROACH 3: HYBRID MULTI-AGENT (âœ… OUR CHOICE)
â”œâ”€â”€ Architecture: User â†’ 5 Agents Pipeline (mostly rule-based) â†’ LLM fallback
â”œâ”€â”€ âœ… Pros:
â”‚  â”œâ”€ Fast: 50-100ms for 90% of requests
â”‚  â”œâ”€ Cheap: ~$50/month (10% LLM usage)
â”‚  â”œâ”€ Reliable: Rule-based for core, flexible fallback
â”‚  â”œâ”€ Scalable: Most requests = no LLM needed
â”‚  â”œâ”€ Maintainable: Clear agent responsibilities
â”‚  â””â”€ Interpretable: Can explain decisions
â”œâ”€â”€ âœ… Cons:
â”‚  â”œâ”€ More complex to build
â”‚  â”œâ”€ Need to maintain rules
â”‚  â””â”€ Training required for edge cases
â”œâ”€â”€ Our design: 90% rule-based, 10% LLM
â””â”€â”€ Cost/month: ~$50-100 (mostly infrastructure)

COMPARISON TABLE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric      â”‚ LLM-Only     â”‚ Rule-Based   â”‚ Hybrid âœ…     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Speed       â”‚ 1000-3000ms  â”‚ 50-100ms     â”‚ 100-300ms     â”‚
â”‚ Cost        â”‚ $100-500/mo  â”‚ $0 +eng time â”‚ $50-100/mo    â”‚
â”‚ Reliability â”‚ 85%          â”‚ 95%          â”‚ 95%           â”‚
â”‚ Flexibility â”‚ Very high    â”‚ Very low     â”‚ High          â”‚
â”‚ Scalability â”‚ Limited      â”‚ Unlimited    â”‚ Unlimited     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Conclusion: Hybrid is best balance for production
```

---

### 2.2 LLM Model Selection

**Comparison: OpenAI, Anthropic, Qwen vs Self-Hosted**

```
OPENAI (GPT-4, GPT-3.5)
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Best quality
â”‚  â”œâ”€ Matured API
â”‚  â””â”€ Good docs
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Expensive: $10-15 per million tokens
â”‚  â”œâ”€ No data privacy (data sent to OpenAI)
â”‚  â”œâ”€ Rate limiting
â”‚  â””â”€ Need stable internet
â””â”€â”€ Cost estimate: $0.01+ per request

ANTHROPIC (Claude)
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Good quality
â”‚  â”œâ”€ Better context handling (100k tokens)
â”‚  â””â”€ Responsible AI focus
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Expensive: Similar to OpenAI
â”‚  â”œâ”€ Limited availability (no Vietnam endpoint)
â”‚  â””â”€ Slower API
â””â”€â”€ Cost estimate: $0.01+ per request

QWEN (Alibaba - âœ… OUR CHOICE)
â”œâ”€â”€ Used: Qwen2.5-3B-Instruct (locally)
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Free (open-source)
â”‚  â”œâ”€ Data privacy (runs locally)
â”‚  â”œâ”€ Fast (small model)
â”‚  â”œâ”€ Optimized for Chinese/Vietnamese
â”‚  â””â”€ No rate limiting
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Lower quality than GPT-4
â”‚  â”œâ”€ Need local GPU/server
â”‚  â”œâ”€ Less official support
â”‚  â””â”€ Smaller community
â””â”€â”€ Cost estimate: $0 (hardware cost only)

SELF-HOSTED OPTIONS
â”œâ”€â”€ Llama 2
â”œâ”€â”€ Mistral
â”œâ”€â”€ OpenLlama
â””â”€â”€ All pros/cons similar to Qwen

COMPARISON TABLE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model      â”‚ Quality    â”‚ Cost        â”‚ Privacy  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4      â”‚ Excellent  â”‚ $$$         â”‚ Low      â”‚
â”‚ Claude     â”‚ Excellent  â”‚ $$$         â”‚ Low      â”‚
â”‚ Qwen âœ…    â”‚ Good       â”‚ Free        â”‚ High âœ…  â”‚
â”‚ Llama      â”‚ Good       â”‚ Free        â”‚ High âœ…  â”‚
â”‚ Mistral    â”‚ Good       â”‚ Free        â”‚ High âœ…  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Decision: Qwen because:
- Free (large cost savings)
- Privacy (data stays local)
- Sufficient quality (90% rule-based anyway)
- Vietnamese optimized
- Works offline if needed
```

---

## ğŸ“Š III. Lá»°A CHá»ŒN RECOMMENDATION ALGORITHM

### 3.1 Algorithm Comparison

```
ALGORITHM 1: CONTENT-BASED ONLY
â”œâ”€â”€ How: Find products similar to viewed items
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ No cold start (works for new users)
â”‚  â”œâ”€ Interpretable (easy to explain)
â”‚  â”œâ”€ No user data needed (privacy)
â”‚  â””â”€ Fast (simple computation)
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Limited discovery (only similar products)
â”‚  â”œâ”€ Boring (repetitive recommendations)
â”‚  â”œâ”€ No user-user learning
â”‚  â””â”€ Easy to game (just update attributes)
â””â”€â”€ Use case: Very new user with no history

ALGORITHM 2: COLLABORATIVE FILTERING ONLY
â”œâ”€â”€ How: Recommend what similar users bought
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Discovery (find new items users like)
â”‚  â”œâ”€ Interesting (diverse recommendations)
â”‚  â”œâ”€ Works for mature users
â”‚  â””â”€ Learns user patterns well
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Cold start (new users/products problem)
â”‚  â”œâ”€ Sparsity (few interactions early on)
â”‚  â”œâ”€ Popularity bias (always recommend bestsellers)
â”‚  â””â”€ Computational cost (large matrices)
â””â”€â”€ Use case: Platform with lots of users

ALGORITHM 3: MATRIX FACTORIZATION
â”œâ”€â”€ How: Decompose user-product matrix (SVD, NMF)
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Handles sparsity well
â”‚  â”œâ”€ Reduces computation
â”‚  â”œâ”€ Good accuracy
â”‚  â””â”€ Scalable
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Still cold start problem
â”‚  â”œâ”€ Complex to implement
â”‚  â”œâ”€ Requires tuning
â”‚  â””â”€ Black box (hard to explain)
â””â”€â”€ Use case: Large-scale recommendation systems

ALGORITHM 4: DEEP LEARNING (Neural CF)
â”œâ”€â”€ How: Neural networks for user-item interactions
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ State-of-art accuracy
â”‚  â”œâ”€ Handles non-linear patterns
â”‚  â”œâ”€ Can incorporate multiple features
â”‚  â””â”€ End-to-end learning
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Requires lots of data
â”‚  â”œâ”€ Computationally expensive
â”‚  â”œâ”€ Black box (not interpretable)
â”‚  â”œâ”€ Overfitting risk
â”‚  â””â”€ Complex to implement
â””â”€â”€ Use case: If you have 100k+ users

ALGORITHM 5: HYBRID (âœ… OUR CHOICE)
â”œâ”€â”€ How: Combine multiple algorithms (ensemble)
â”œâ”€â”€ Formula: Score = w1Ã—Content + w2Ã—Collaborative
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Combines strength of both
â”‚  â”œâ”€ Content handles cold start
â”‚  â”œâ”€ Collaborative provides discovery
â”‚  â”œâ”€ Better accuracy than single method
â”‚  â”œâ”€ Interpretable (explain both components)
â”‚  â””â”€ Handles popularity bias
â”œâ”€â”€ Cons:
â”‚  â”œâ”€ Need to tune weights
â”‚  â”œâ”€ Slightly more complex
â”‚  â””â”€ More data to compute
â””â”€â”€ Perfect for: Medium-scale e-commerce

COMPARISON TABLE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Algorithm    â”‚ Accuracyâ”‚ Cold Startâ”‚ Scalability â”‚ Complexity â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Content-only â”‚ Fair    â”‚ âœ… Good  â”‚ âœ… High     â”‚ Low        â”‚
â”‚ Collab-only  â”‚ Good    â”‚ âŒ Bad   â”‚ Fair        â”‚ Medium     â”‚
â”‚ Matrix Fact. â”‚ Good    â”‚ Fair     â”‚ Good        â”‚ Medium     â”‚
â”‚ Deep Learningâ”‚ âœ… Best â”‚ âŒ Bad   â”‚ Fair        â”‚ âœ… High    â”‚
â”‚ Hybrid âœ…    â”‚ âœ… Good â”‚ âœ… Good  â”‚ âœ… Good     â”‚ Medium     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Decision: Hybrid is optimal for our scale
- 100-1000 active users â†’ Hybrid perfect
- Need good accuracy + cold start handling
- Interpretable to stakeholders
- Not too complex for small team
```

---

### 3.2 Similarity Metrics

```
METRIC 1: EUCLIDEAN DISTANCE
â”œâ”€â”€ Formula: sqrt(Î£(x_i - y_i)Â²)
â”œâ”€â”€ Pros: Intuitive, simple
â”œâ”€â”€ Cons: Sensitive to magnitude, slow
â””â”€â”€ Use: Low-dimensional data

METRIC 2: COSINE SIMILARITY (âœ… COMMONLY USED)
â”œâ”€â”€ Formula: cos(Î¸) = (AÂ·B) / (|A||B|)
â”œâ”€â”€ Range: -1 to 1 (usually 0 to 1 for products)
â”œâ”€â”€ Pros:
â”‚  â”œâ”€ Insensitive to magnitude
â”‚  â”œâ”€ Fast to compute
â”‚  â”œâ”€ Works well for text/categories
â”‚  â””â”€ Interpretable (angle between vectors)
â”œâ”€â”€ Cons: Doesn't capture magnitude
â””â”€â”€ Use: Text, sparse data, our choice!

METRIC 3: JACCARD SIMILARITY
â”œâ”€â”€ Formula: |A âˆ© B| / |A âˆª B|
â”œâ”€â”€ Range: 0 to 1
â”œâ”€â”€ Pros: Works for sets
â”œâ”€â”€ Cons: Binary (doesn't capture strength)
â””â”€â”€ Use: Tag matching, categorical data

METRIC 4: PEARSON CORRELATION
â”œâ”€â”€ Formula: Cov(A,B) / (Ïƒ_A Ã— Ïƒ_B)
â”œâ”€â”€ Pros: Handles trends
â”œâ”€â”€ Cons: Requires rating data
â””â”€â”€ Use: Rating-based systems

Our Choice: Cosine Similarity
â”œâ”€â”€ Why:
â”‚  â”œâ”€ Fast computation
â”‚  â”œâ”€ Works well for categories/tags
â”‚  â”œâ”€ Handles sparse vectors well
â”‚  â””â”€ Industry standard (Netflix, Amazon)
â”‚
â””â”€â”€ Example:
    Product A: [backpack: 1, red: 1, leather: 0]
    Product B: [backpack: 1, red: 1, leather: 0]
    Similarity = 1.0 (identical)
    
    Product A: [backpack: 1, red: 1, leather: 0]
    Product C: [shoes: 0, red: 1, blue: 1]
    Similarity = 0.33 (different categories)
```

---

## ğŸ—„ï¸ IV. DATABASE DESIGN CHOICES

### 4.1 Normalization Level

```
DENORMALIZATION (âŒ Not used)
â”œâ”€â”€ Store all data in few tables
â”œâ”€â”€ Pros: Fast reads (no joins)
â”œâ”€â”€ Cons: Duplicate data, hard to update
â””â”€â”€ Example: Each product has entire category details copied

NORMAL FORM 1 (Rarely useful)
â”œâ”€â”€ Remove repeating groups
â”œâ”€â”€ Still lots of duplication

NORMAL FORM 2-3 (âœ… OUR CHOICE)
â”œâ”€â”€ Remove partial dependencies
â”œâ”€â”€ Remove transitive dependencies
â””â”€â”€ Clean, organized structure

NORMAL FORM 4-5 (Over-normalized)
â”œâ”€â”€ Rare edge cases
â”œâ”€â”€ Makes queries complex
â”œâ”€â”€ Not needed unless specific reason

Our Database Design:

âœ… Products are normalized:
   product {id, title, category_id, ...}
   category {id, name, description}
   â† Separate category to avoid duplication

âœ… User interactions are denormalized (intentionally):
   rec_user_interactions {
     user_id,
     product_id,
     product_handle,     â† Denormalized for speed
     interaction_type,
     metadata (JSONB)    â† Flexible storage
   }
   â† Reason: This is a log, reads are frequent, updates rare

Principle: Normalize for transactional data (products, orders)
          Denormalize for log data (interactions, analytics)
```

---

### 4.2 Schema Design Pattern

```
OPTION 1: RELATIONAL (Traditional)
â”œâ”€â”€ Separate table for each entity
â”œâ”€â”€ Foreign keys for relationships
â”œâ”€â”€ Strict schema
â”œâ”€â”€ Pros: Data integrity, normalized
â”œâ”€â”€ Cons: Rigid, needs migrations
â””â”€â”€ Used for: medusa tables, orders, products

OPTION 2: DOCUMENT (NoSQL)
â”œâ”€â”€ Store complete objects as JSON/BSON
â”œâ”€â”€ Flexible schema
â”œâ”€â”€ Embedded data
â”œâ”€â”€ Pros: Flexible, fast for nested data
â”œâ”€â”€ Cons: Duplication, hard to query
â””â”€â”€ Would use: MongoDB

OPTION 3: HYBRID (âœ… OUR CHOICE)
â”œâ”€â”€ Relational for structured data
â”œâ”€â”€ JSONB for flexible metadata
â”œâ”€â”€ Best of both worlds
â”œâ”€â”€ Example tables:
â”‚  â”œâ”€ product (relational)
â”‚  â”œâ”€ rec_user_interactions (relational + JSONB)
â”‚  â”‚  â””â”€ metadata JSONB {price, category, custom_attrs}
â”‚  â””â”€ rec_recommendations_cache (relational + JSONB)
â”‚     â””â”€ recommendations JSONB [{id, score, reason}]
â””â”€ Reason: PostgreSQL JSONB + SQL gives us flexibility

Our rec_user_interactions table:

CREATE TABLE rec_user_interactions (
  id TEXT PRIMARY KEY,
  user_id TEXT NOT NULL,
  session_id TEXT,
  product_id TEXT NOT NULL,
  product_handle TEXT NOT NULL,
  interaction_type TEXT NOT NULL,    â† Relational part
  weight FLOAT NOT NULL,
  created_at TIMESTAMP NOT NULL,
  
  -- Flexible part for future attributes
  metadata JSONB,                    â† Can store anything
  
  UNIQUE (user_id, session_id, product_id, interaction_type)
);

Example metadata values:
{
  "price": 1299000,
  "currency": "VND",
  "category": "backpack",
  "tags": ["red", "leather", "student"],
  "custom_field": "any value"
}

Benefits:
âœ… Can add new tracking fields without migrations
âœ… Can store different data per event type
âœ… Still queryable with SQL: WHERE metadata->>'category' = 'backpack'
```

---

### 4.3 Indexing Strategy

```
INDEXES WE USE:

1. User interactions table
   CREATE INDEX idx_user_id_created_at ON rec_user_interactions(user_id, created_at DESC);
   â””â”€ Used for: "Get recent interactions for user"
   
   CREATE INDEX idx_product_id_interaction_type ON rec_user_interactions(product_id, interaction_type);
   â””â”€ Used for: "Find all purchases of product X"
   
   CREATE INDEX idx_session_created_at ON rec_user_interactions(session_id, created_at);
   â””â”€ Used for: "Get session interactions"

2. User preferences table
   CREATE INDEX idx_user_id ON rec_user_preferences(user_id);
   â””â”€ Used for: "Get preferences for user"
   
   CREATE INDEX idx_category_score ON rec_user_preferences(category, score DESC);
   â””â”€ Used for: "Find top categories by score"

3. Product similarities table
   CREATE INDEX idx_product_1_score ON rec_product_similarities(product_id_1, similarity_score DESC);
   â””â”€ Used for: "Find similar products to X"

4. Cache table
   CREATE INDEX idx_user_expires ON rec_recommendations_cache(user_id, expires_at);
   â””â”€ Used for: "Check if cache still valid"

Total indexes: 7-8 strategically placed
â”œâ”€â”€ Each one has clear use case
â”œâ”€â”€ Reduces query time from seconds to milliseconds
â””â”€â”€ Trade-off: Slightly slower writes (OK for our workload)

Index Selection Process:
1. Identify slow queries (from logs)
2. Add index to columns in WHERE clause
3. Measure improvement
4. Remove unused indexes (bloat)
5. Monitor regularly
```

---

## ğŸ” V. SECURITY CONSIDERATIONS

### 5.1 API Security

```
AUTHENTICATION:
â”œâ”€â”€ Medusa Admin: API token in headers
â”œâ”€â”€ Chatbot Service: Session-based (optional for MVP)
â”œâ”€â”€ Recommendation: No auth needed (public data)
â””â”€â”€ Frontend: User session (cookies)

EXAMPLE: Call Medusa API from Chatbot
GET /store/products HTTP/1.1
Host: medusa:9000
Authorization: Bearer {API_KEY}
X-API-KEY: sk_live_...

PROTECTION:
â”œâ”€â”€ API rate limiting
â”œâ”€â”€ Input validation
â”œâ”€â”€ SQL injection prevention (ORM)
â””â”€â”€ CORS headers
```

### 5.2 Data Privacy

```
USER DATA HANDLING:
â”œâ”€â”€ Interactions: Pseudonymized (user_id is UUID, not email)
â”œâ”€â”€ Email: Only when customer logs in
â”œâ”€â”€ Tracking: Session-based for guests
â”œâ”€â”€ GDPR: Can delete user interactions on request
â””â”€â”€ Cache: No sensitive data in Redis

EXAMPLE: Guest tracking
â”œâ”€â”€ Generate: session_id = random_uuid()
â”œâ”€â”€ Track: POST /track {session_id: "sess_123", product_id: "prod_123"}
â”œâ”€â”€ No personal info: Email/name not stored
â””â”€â”€ Privacy: Can't identify individual users

EXAMPLE: Customer tracking
â”œâ”€â”€ Generate: user_id = customer.id
â”œâ”€â”€ Link: Customer logs in
â”œâ”€â”€ Data: Can tie to account
â””â”€â”€ GDPR: Customer can request deletion
```

---

## âš™ï¸ VI. OPERATIONAL DECISIONS

### 6.1 Deployment Strategy

**Deployment Options:**

```
OPTION 1: Local Docker Compose
â”œâ”€â”€ Use: Development, testing
â”œâ”€â”€ Pros: Simple, all services local
â”œâ”€â”€ Cons: Can't scale, poor reliability
â””â”€â”€ Our use: Development environment

OPTION 2: Docker Swarm
â”œâ”€â”€ Use: Small to medium scale
â”œâ”€â”€ Pros: Simple orchestration
â”œâ”€â”€ Cons: Limited features, no auto-scaling
â””â”€â”€ Could use: Production (simple)

OPTION 3: KUBERNETES (âœ… ENTERPRISE CHOICE)
â”œâ”€â”€ Use: Large scale production
â”œâ”€â”€ Pros: Auto-scaling, rolling updates, self-healing
â”œâ”€â”€ Cons: Complex, requires expertise
â””â”€â”€ Recommendation: Migrate here for production

OPTION 4: MANAGED SERVICES
â”œâ”€â”€ Use: Cloud-native
â”œâ”€â”€ Examples: AWS ECS, Google Cloud Run, Azure Container Instances
â”œâ”€â”€ Pros: Managed infrastructure, scaling
â”œâ”€â”€ Cons: Vendor lock-in, cost
â””â”€â”€ Could use: If hosted on cloud

Our Choice: Docker Compose (for now)
â”œâ”€â”€ Why: Development/demo stage
â”œâ”€â”€ Transition path: Docker Compose â†’ Kubernetes for production
â”œâ”€â”€ Easy to understand and modify
â”œâ”€â”€ Good enough for graduation project
```

---

### 6.2 Database Backup Strategy

```
BACKUP LEVELS:

Level 1: DAILY BACKUPS (Minimum)
â”œâ”€â”€ Schedule: Daily at 2 AM
â”œâ”€â”€ Retention: 7 days
â”œâ”€â”€ Method: pg_dump to file
â””â”€â”€ Command:
    pg_dump medusa-store > backup_$(date +%Y%m%d).sql

Level 2: HOURLY BACKUPS (Critical data)
â”œâ”€â”€ Schedule: Every hour
â”œâ”€â”€ Retention: 24 hours
â”œâ”€â”€ Method: PostgreSQL WAL (Write-Ahead Logging)
â””â”€â”€ Stores: All transactions

Level 3: REAL-TIME REPLICATION (Disaster recovery)
â”œâ”€â”€ Schedule: Continuous
â”œâ”€â”€ Replication: To standby server
â”œâ”€â”€ Method: Streaming replication
â””â”€â”€ Failover: Automatic (if available)

Our approach (MVP):
â”œâ”€â”€ Daily backups to local storage
â”œâ”€â”€ Manual backups before major changes
â”œâ”€â”€ Restore procedure tested quarterly
â””â”€ Upgrade to Level 2-3 for production
```

---

### 6.3 Monitoring & Logging

```
WHAT TO MONITOR:

1. Application Metrics
   â”œâ”€ API response times (p50, p95, p99)
   â”œâ”€ Error rates
   â”œâ”€ Request volume
   â”œâ”€ Cache hit rate
   â””â”€ Recommendation quality (CTR, conversion)

2. Infrastructure Metrics
   â”œâ”€ CPU usage
   â”œâ”€ Memory usage
   â”œâ”€ Disk space
   â”œâ”€ Network I/O
   â””â”€ Container health

3. Database Metrics
   â”œâ”€ Query times
   â”œâ”€ Connection count
   â”œâ”€ Cache evictions
   â””â”€ Disk usage

TOOLS:
â”œâ”€â”€ Local: Docker built-in stats
â”œâ”€â”€ Prometheus: Metrics collection
â”œâ”€â”€ Grafana: Visualization
â”œâ”€â”€ ELK Stack: Logging (Elasticsearch, Logstash, Kibana)
â””â”€â”€ Sentry: Error tracking

Our approach (MVP):
â”œâ”€â”€ Docker logs (docker-compose logs)
â”œâ”€â”€ PostgreSQL slow query log
â”œâ”€â”€ Application logging to stdout
â””â”€ Upgrade to Prometheus+Grafana for production
```

---

## ğŸ¯ VII. COST ANALYSIS

### 7.1 Monthly Infrastructure Cost (Estimate)

```
LOCAL DEVELOPMENT (0 cost):
â”œâ”€â”€ Your laptop: Use existing resources
â””â”€â”€ Total: $0

PRODUCTION ESTIMATE (Self-hosted):

Server:
â”œâ”€â”€ Frontend (Next.js): 1 CPU, 2GB RAM = $10-20/mo
â”œâ”€â”€ Medusa Backend: 2 CPU, 4GB RAM = $20-40/mo
â”œâ”€â”€ Chatbot Service: 2 CPU, 4GB RAM = $20-40/mo
â”œâ”€â”€ Recommendation Service: 2 CPU, 4GB RAM = $20-40/mo
â””â”€â”€ Database (PostgreSQL): 4 CPU, 8GB RAM = $40-80/mo
    â”œâ”€ SSD: 100GB = $5-10/mo
    â””â”€ Backup: External storage = $5-10/mo

Total Server Cost: ~$120-240/month

Other Costs:
â”œâ”€â”€ LLM API (Qwen local): $0
â”œâ”€â”€ Redis: Included in server
â”œâ”€â”€ Networking: $5-10/mo
â”œâ”€â”€ Domain: $10-15/year
â””â”€â”€ SSL Certificate: $0-12/year

Total Monthly: ~$125-250/month

vs CLOUD PROVIDERS:

AWS Estimate:
â”œâ”€â”€ EC2 instances: $150-300/mo
â”œâ”€â”€ RDS (PostgreSQL): $50-100/mo
â”œâ”€â”€ ElastiCache (Redis): $20-40/mo
â”œâ”€â”€ Data transfer: $10-20/mo
â””â”€â”€ Total: ~$230-460/mo

Google Cloud Estimate:
â”œâ”€â”€ Compute Engine: $150-250/mo
â”œâ”€â”€ Cloud SQL: $50-100/mo
â”œâ”€â”€ Cloud Memorystore: $20-40/mo
â””â”€â”€ Total: ~$220-390/mo

Conclusion:
âœ… Self-hosted (Docker Compose): $125-250/mo cheapest
âŒ Cloud: $230-460/mo (2-3x more expensive)

For production:
â”œâ”€â”€ Small scale (<100K users): Self-hosted
â”œâ”€â”€ Medium scale (100K-1M users): Cloud (easier scaling)
â””â”€â”€ Large scale (>1M users): Kubernetes + cloud
```

---

### 7.2 LLM Cost Analysis

```
SCENARIO: 1000 users, 100 daily chats

DEFAULT (LLM-only approach):
â”œâ”€â”€ 100 chats Ã— 30 days = 3000 chats/month
â”œâ”€â”€ Avg tokens per chat: 500 (input) + 300 (output) = 800
â”œâ”€â”€ Total tokens: 3000 Ã— 800 = 2.4M tokens/month
â”œâ”€â”€ OpenAI price: $0.005 per 1K tokens input + $0.015 output
â”œâ”€â”€ Cost: (3000 Ã— 500 Ã— 0.005) + (3000 Ã— 300 Ã— 0.015) = $22.5/mo

HYBRID APPROACH (Our choice):
â”œâ”€â”€ 90% rule-based: 2700 chats with 0 LLM cost = $0
â”œâ”€â”€ 10% LLM fallback: 300 chats Ã— 800 tokens = 240K tokens
â”œâ”€â”€ Cost: (300 Ã— 500 Ã— 0.005) + (300 Ã— 300 Ã— 0.015) = $2.25/mo
â”œâ”€â”€ Qwen local: $0 (already have GPU)
â””â”€â”€ Actual cost: ~$0 (negligible)

Savings: $22.5 - $0 = $22.5/month per 1000 users
Annual savings: $270 for 1000 users

At scale (100K users):
â”œâ”€â”€ LLM-only: ~$2250/month
â”œâ”€â”€ Hybrid: ~$0/month (with local Qwen)
â””â”€â”€ Savings: $27,000/year!

This is why hybrid architecture is critical!
```

---

## ğŸ“ˆ VIII. SCALABILITY ROADMAP

### 8.1 From MVP to Production

```
STAGE 1: MVP (Current)
â”œâ”€â”€ Users: <1000
â”œâ”€â”€ DAU: <100
â”œâ”€â”€ Infrastructure: Docker Compose local
â”œâ”€â”€ Database: Single PostgreSQL instance
â”œâ”€â”€ Cache: Redis local
â”œâ”€â”€ Scaling: Manual (add more containers)
â””â”€â”€ Estimated timeline: Now

STAGE 2: Beta (6 months)
â”œâ”€â”€ Users: 1000-10000
â”œâ”€â”€ DAU: 100-1000
â”œâ”€â”€ Infrastructure: Docker Compose on cloud VM
â”œâ”€â”€ Database: Managed PostgreSQL (AWS RDS)
â”œâ”€â”€ Cache: Managed Redis (ElastiCache)
â”œâ”€â”€ Scaling: Docker Compose scaling, need monitoring
â”œâ”€â”€ Cost: ~$200-300/month
â””â”€â”€ Improvements needed:
    â”œâ”€ Error handling
    â”œâ”€ Logging/monitoring
    â””â”€ Performance tuning

STAGE 3: Scale (1 year)
â”œâ”€â”€ Users: 10K-100K
â”œâ”€â”€ DAU: 1K-10K
â”œâ”€â”€ Infrastructure: Kubernetes cluster
â”œâ”€â”€ Database: PostgreSQL with read replicas
â”œâ”€â”€ Cache: Multi-instance Redis
â”œâ”€â”€ Scaling: Auto-scaling groups, load balancing
â”œâ”€â”€ Cost: ~$1000-2000/month
â””â”€â”€ Improvements needed:
    â”œâ”€ Microservice mesh (Istio)
    â”œâ”€ Distributed tracing
    â”œâ”€ Advanced analytics
    â””â”€ Multi-region deployment

STAGE 4: Enterprise (2+ years)
â”œâ”€â”€ Users: 100K+
â”œâ”€â”€ DAU: 10K+
â”œâ”€â”€ Infrastructure: Multi-cloud, multi-region
â”œâ”€â”€ Database: Distributed database (CockroachDB, Spanner)
â”œâ”€â”€ Cache: Global Redis cluster
â”œâ”€â”€ Scaling: Global load balancing
â”œâ”€â”€ Cost: $5K-10K+/month
â””â”€â”€ Improvements needed:
    â”œâ”€ Machine learning pipelines
    â”œâ”€ Real-time analytics
    â”œâ”€ Advanced personalization
    â””â”€ Compliance & security
```

### 8.2 Performance Optimization Roadmap

```
CURRENT (MVP):
â”œâ”€â”€ Response time: Acceptable (200-500ms)
â”œâ”€â”€ Cache hit: ~80%
â”œâ”€â”€ P95 latency: ~500ms
â””â”€â”€ Status: âœ… Sufficient for MVP

QUICK WINS (Next month):
â”œâ”€â”€ Add database indexes: 50% faster queries
â”œâ”€â”€ Implement Redis caching properly: 80% â†’ 90% hit rate
â”œâ”€â”€ Compress API responses: 30% smaller payloads
â”œâ”€â”€ Lazy load components: Faster page load
â””â”€ Expected improvement: 2-3x faster overall

MEDIUM TERM (3 months):
â”œâ”€â”€ Connection pooling: Reduce DB connection overhead
â”œâ”€â”€ Query optimization: Identify slow queries
â”œâ”€â”€ Batch operations: Combine multiple API calls
â”œâ”€â”€ CDN for static assets: Global distribution
â””â”€ Expected improvement: 3-5x faster

LONG TERM (6+ months):
â”œâ”€â”€ Caching layer redesign: More aggressive caching
â”œâ”€â”€ Database partitioning: Split large tables
â”œâ”€â”€ Search optimization: Dedicated search service
â”œâ”€â”€ Machine learning improvements: Better recommendations
â””â”€ Expected improvement: 5-10x faster
```

---

**Document Version:** v1.0  
**Last Updated:** December 15, 2025  
**Status:** Production-Ready âœ…
